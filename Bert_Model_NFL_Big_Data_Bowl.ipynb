{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle competition: NFL Big Data Bowl\n",
    "- Refer to [How many yards will an NFL player gain after receiving a handoff?](https://www.kaggle.com/c/nfl-big-data-bowl-2020)\n",
    "- BERT: Bidirectional Encoder Representations from Transformers\n",
    "- Pytorch Transformer model\n",
    "- The notebook was originally obtained from [the kaggle soltuion notebook](https://www.kaggle.com/nyanpn/pytorch-transformer-public-14th-private-22nd)\n",
    "- Refer to https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/119314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- Load data and prepare features\n",
    "- Prepare a customized data loader for pytorch models\n",
    "- Define functions and debug them one by one\n",
    "- Split data sets into train and validation\n",
    "- Define Transformer model\n",
    "- Parse options and instantiate a model with parsed options\n",
    "- Train models on GPU/Cuda\n",
    "- Ensemble through Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import time\n",
    "from typing import Optional, Tuple, List, Dict, Type\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn.modules.container import ModuleList\n",
    "#\n",
    "from transformers.modeling_bert import BertConfig, BertEncoder, BertModel\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  and prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Prep and Feature Engineering\n",
    "#######################################################################################################################\n",
    "\n",
    "# Thanks to: https://www.kaggle.com/cpmpml/initial-wrangling-voronoi-areas-in-python\n",
    "# new: adjusted features\n",
    "    #df.loc[:, 'S'] = 10 * df['Dis']    \n",
    "    #df.loc[df.Season == 2017, 'A'] = df[df.Season == 2018]['A'].mean()\n",
    "    \n",
    "def prep(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    print(f\"prepare features from source data with function: prep\")       \n",
    "    df['ToLeft'] = df.PlayDirection == \"left\"\n",
    "    df['IsBallCarrier'] = df.NflId == df.NflIdRusher\n",
    "\n",
    "    df.loc[df.VisitorTeamAbbr == \"ARI\", 'VisitorTeamAbbr'] = \"ARZ\"\n",
    "    df.loc[df.HomeTeamAbbr == \"ARI\", 'HomeTeamAbbr'] = \"ARZ\"\n",
    "\n",
    "    df.loc[df.VisitorTeamAbbr == \"BAL\", 'VisitorTeamAbbr'] = \"BLT\"\n",
    "    df.loc[df.HomeTeamAbbr == \"BAL\", 'HomeTeamAbbr'] = \"BLT\"\n",
    "\n",
    "    df.loc[df.VisitorTeamAbbr == \"CLE\", 'VisitorTeamAbbr'] = \"CLV\"\n",
    "    df.loc[df.HomeTeamAbbr == \"CLE\", 'HomeTeamAbbr'] = \"CLV\"\n",
    "\n",
    "    df.loc[df.VisitorTeamAbbr == \"HOU\", 'VisitorTeamAbbr'] = \"HST\"\n",
    "    df.loc[df.HomeTeamAbbr == \"HOU\", 'HomeTeamAbbr'] = \"HST\"\n",
    "    \n",
    "    ## adjusted features from kaggle discussions\n",
    "    ##06/06/2020\n",
    "    df.loc[:, 'S'] = 10 * df['Dis']    \n",
    "    df.loc[df.Season == 2017, 'A'] = df[df.Season == 2018]['A'].mean()\n",
    "    \n",
    "    # standardization\n",
    "    df['TeamOnOffense'] = \"home\"\n",
    "    df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n",
    "    df['IsOnOffense'] = df.Team == df.TeamOnOffense  # Is player on offense?\n",
    "    df['YardLine_std'] = 100 - df.YardLine\n",
    "    df.loc[df.FieldPosition.fillna('') == df.PossessionTeam, 'YardLine_std'] = df.loc[\n",
    "        df.FieldPosition.fillna('') == df.PossessionTeam, 'YardLine']\n",
    "    df['X_std'] = df.X\n",
    "    df.loc[df.ToLeft, 'X_std'] = 120 - df.loc[df.ToLeft, 'X']\n",
    "    df['Y_std'] = df.Y\n",
    "    df.loc[df.ToLeft, 'Y_std'] = 160 / 3 - df.loc[df.ToLeft, 'Y']\n",
    "    df['Orientation_std'] = df.Orientation\n",
    "    df.loc[df.ToLeft, 'Orientation_std'] = np.mod(180 + df.loc[df.ToLeft, 'Orientation_std'], 360)\n",
    "    df['Dir_std'] = df.Dir\n",
    "    df.loc[df.ToLeft, 'Dir_std'] = np.mod(180 + df.loc[df.ToLeft, 'Dir_std'], 360)\n",
    "    df['IsOffence'] = df['Team'] == df['TeamOnOffense']\n",
    "\n",
    "    # translate Home/Visitor to Offence/Defense\n",
    "    df['OffenceScoreBeforePlay'] = df['HomeScoreBeforePlay']\n",
    "    df.loc[df.TeamOnOffense == \"away\", 'OffenceScoreBeforePlay'] = df.loc[\n",
    "        df.TeamOnOffense == \"away\", 'VisitorScoreBeforePlay']\n",
    "    df['DefenseScoreBeforePlay'] = df['VisitorScoreBeforePlay']\n",
    "    df.loc[df.TeamOnOffense == \"away\", 'DefenseScoreBeforePlay'] = df.loc[\n",
    "        df.TeamOnOffense == \"away\", 'HomeScoreBeforePlay']\n",
    "\n",
    "    df['OffenceTeamAbbr'] = df['HomeTeamAbbr']\n",
    "    df.loc[df.TeamOnOffense == \"away\", 'OffenceTeamAbbr'] = df.loc[df.TeamOnOffense == \"away\", 'VisitorTeamAbbr']\n",
    "    df['DefenseTeamAbbr'] = df['VisitorTeamAbbr']\n",
    "    df.loc[df.TeamOnOffense == \"away\", 'DefenseTeamAbbr'] = df.loc[df.TeamOnOffense == \"away\", 'HomeTeamAbbr']\n",
    "\n",
    "    df['Year'] = pd.to_datetime(df.TimeSnap).dt.year\n",
    "    df.loc[df['Year'] == 2017, 'Orientation_std'] = np.mod(90 + df.loc[df['Year'] == 2017, 'Orientation_std'], 360)\n",
    "\n",
    "    player_features = ['X_std', 'Y_std', 'S', 'A', 'Dis', 'Orientation_std', 'Dir_std', 'NflId', 'JerseyNumber',\n",
    "                       'PlayerHeight',\n",
    "                       'PlayerWeight', 'PlayerBirthDate', 'PlayerCollegeName', 'Position', 'IsBallCarrier', 'IsOffence']\n",
    "\n",
    "    play_features = ['YardLine_std', 'Quarter', 'GameClock', 'PossessionTeam', 'Down', 'Distance', 'FieldPosition',\n",
    "                     'OffenceScoreBeforePlay', 'DefenseScoreBeforePlay', 'OffenseFormation', 'OffensePersonnel',\n",
    "                     'DefendersInTheBox', 'DefensePersonnel', 'TimeHandoff', 'TimeSnap',\n",
    "                     'OffenceTeamAbbr', 'DefenseTeamAbbr', 'Week', 'Stadium', 'Location',\n",
    "                     'Turf', 'GameWeather', 'Temperature', 'Humidity', 'WindSpeed', 'WindDirection',\n",
    "                     'TeamOnOffense', 'HomeTeamAbbr', 'Year']\n",
    "\n",
    "    if 'Yards' in df:\n",
    "        play_features.append('Yards')\n",
    "\n",
    "    players = df[['PlayId', 'GameId'] + player_features].copy()\n",
    "\n",
    "    play = df[['GameId', 'PlayId'] + play_features].copy().drop_duplicates(subset=['PlayId'])\n",
    "\n",
    "    return play, players\n",
    "\n",
    "\n",
    "def prep_players_nn(play, players, scaler=None, scaler_meta=None):\n",
    "    print(f\"prepare player data with function: prep_players_nn\")    \n",
    "    p = players.drop(['NflId', 'JerseyNumber', 'PlayerHeight', 'PlayerBirthDate', 'PlayerCollegeName', 'GameId'],\n",
    "                     axis=1).copy()\n",
    "\n",
    "    if scaler is None:\n",
    "        is_training = True\n",
    "    else:\n",
    "        assert scaler_meta is not None\n",
    "        is_training = False\n",
    "\n",
    "    if 'YardLine_std' not in p:\n",
    "        p = pd.merge(p, play[['YardLine_std', 'PlayId']], on='PlayId', how='left')\n",
    "\n",
    "    p['IsBallCarrier'] = p['IsBallCarrier'].astype(int)\n",
    "    p['IsOffence'] = p['IsOffence'].astype(int)\n",
    "    p['X_std'] -= p['YardLine_std']\n",
    "\n",
    "    p['Dir_cos'] = np.cos(np.deg2rad(90 - p['Dir_std']))\n",
    "    p['Dir_sin'] = np.sin(np.deg2rad(90 - p['Dir_std']))\n",
    "    \n",
    "    ## fill in missing with 0\n",
    "    p = p.fillna(0)\n",
    "\n",
    "    rb = p[p['IsBallCarrier'] == 1][['X_std', 'Y_std', 'PlayId', 'S', 'Dir_sin', 'Dir_cos', 'Dir_std']]\n",
    "    rb.columns = ['X_', 'Y_', 'PlayId', 'S_', 'Dir_sin_', 'Dir_cos_', 'Dir_std_']\n",
    "\n",
    "    p = pd.merge(p, rb, how='left')\n",
    "    p['DX'] = p['X_std'] - p['X_']\n",
    "    p['DY'] = p['Y_std'] - p['Y_']\n",
    "\n",
    "    # Relative angle from on Rusher's vector\n",
    "    angles = 90.0 - np.rad2deg(np.arctan2(p['DY'], p['DX']))\n",
    "    p['AngleFromRB'] = angles - p['Dir_std_']\n",
    "    p['AngleFromRB'] = np.mod(p['AngleFromRB'] + 360, 360)\n",
    "    p.loc[p['AngleFromRB'] > 180, 'AngleFromRB'] -= 360\n",
    "\n",
    "    # dTheta of AngleFromRB\n",
    "    dt = 0.001\n",
    "    p['DX2'] = (p['X_std'] + dt * p['Dir_cos'] * p['S']) - (p['X_'] + dt * p['Dir_cos_'] * p['S_'])\n",
    "    p['DY2'] = (p['Y_std'] + dt * p['Dir_sin'] * p['S']) - (p['Y_'] + dt * p['Dir_sin_'] * p['S_'])\n",
    "\n",
    "    angles = 90.0 - np.rad2deg(np.arctan2(p['DY2'], p['DX2']))\n",
    "    p['AngleFromRB2'] = angles - p['Dir_std_']\n",
    "    p['AngleFromRB2'] = np.mod(p['AngleFromRB2'] + 360, 360)\n",
    "    p.loc[p['AngleFromRB2'] > 180, 'AngleFromRB2'] -= 360\n",
    "    p['AngleFromRB2'] = p['AngleFromRB2'] - p['AngleFromRB']\n",
    "    p.loc[p['AngleFromRB2'] > 180, 'AngleFromRB2'] -= 360\n",
    "    p.loc[p['AngleFromRB2'] < -180, 'AngleFromRB2'] += 360\n",
    "\n",
    "    p.loc[p['IsBallCarrier'] == 1, 'AngleFromRB'] = 0\n",
    "    p.loc[p['IsBallCarrier'] == 1, 'AngleFromRB2'] = 0\n",
    "\n",
    "    p.drop(['DX2', 'DY2'], axis=1, inplace=True)\n",
    "\n",
    "    p['AngleTan'] = np.arctan2(p['DY'], p['DX'])\n",
    "\n",
    "    p = p.replace([np.inf, -np.inf], np.nan)\n",
    "    p = p.fillna(0)\n",
    "    p.drop(['X_', 'Y_', 'Dir_sin_', 'Dir_cos_', 'S_', 'Dir_std_', 'Orientation_std', 'Dir_std'], axis=1, inplace=True)\n",
    "\n",
    "    concat = p.drop(['Position', 'PlayId', 'YardLine_std'], axis=1)\n",
    "\n",
    "    if is_training:\n",
    "        scaler = StandardScaler()\n",
    "        scaled = scaler.fit_transform(concat.values)\n",
    "    else:\n",
    "        scaled = scaler.transform(concat.values)\n",
    "    df = pd.DataFrame(scaled, columns=concat.columns)\n",
    "\n",
    "    df_rusher = df[p['IsBallCarrier'] == 1].reset_index(drop=True)\n",
    "    drop_cols = ['IsBallCarrier', 'IsOffence', 'OffenseDist0', 'PlayerWeight', 'DX', 'DY', 'AngleFromRB', 'AngleToRB',\n",
    "                 'DistDelta', 'AngleTan', 'AngleFromRB2', 'delaunay_adj']\n",
    "    drop_cols = [c for c in drop_cols if c in df_rusher.columns]\n",
    "    df_rusher.drop(drop_cols, axis=1, inplace=True)\n",
    "    df_rusher = df_rusher.set_index(players['PlayId'].iloc[np.arange(0, len(players), 22)])\n",
    "\n",
    "    # meta features\n",
    "    meta = play[['YardLine_std', 'Distance']].copy()\n",
    "    meta = meta.replace([np.inf, -np.inf], np.nan)\n",
    "    meta = meta.fillna(0) ## fill in missing with 0\n",
    "\n",
    "    if scaler_meta is not None:\n",
    "        scaled_meta = scaler_meta.transform(meta.values)\n",
    "    else:\n",
    "        scaler_meta = StandardScaler()\n",
    "        scaled_meta = scaler_meta.fit_transform(meta.values)\n",
    "    scaled_meta = pd.DataFrame(scaled_meta, columns=meta.columns)\n",
    "\n",
    "    return df.set_index(players['PlayId']), df_rusher, scaled_meta, scaler, scaler_meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A user-defined data class for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, players: torch.Tensor, rusher: torch.Tensor, meta: torch.Tensor, y: Optional[torch.Tensor],\n",
    "                 yardLine: np.ndarray, year: np.ndarray, player_cols: List[str], rusher_cols: List[str],\n",
    "                 meta_cols: List[str]):\n",
    "        ## inputs with tensor and ndarray\n",
    "        self.players = players\n",
    "        self.rusher = rusher\n",
    "        self.meta = meta\n",
    "        self.y = y\n",
    "        self.yardLine = yardLine\n",
    "        self.year = year\n",
    "        self.player_cols = player_cols\n",
    "        self.rusher_cols = rusher_cols\n",
    "        self.meta_cols = meta_cols\n",
    "\n",
    "        assert self.players.size(0) == self.rusher.size(0)\n",
    "        if yardLine is not None:\n",
    "            assert len(yardLine) == self.players.size(0)\n",
    "\n",
    "    def len(self):\n",
    "        return self.players.size(0)\n",
    "\n",
    "    def y_soft(self, sigma: float = 1.0):\n",
    "        from scipy.ndimage.filters import gaussian_filter1d\n",
    "        return torch.from_numpy(gaussian_filter1d(self.y.numpy(), sigma=sigma))\n",
    "\n",
    "    def slice(self, begin: int, end: int) -> 'Data':\n",
    "        ## inputs:(p, r, m, yd, yr) and target: y\n",
    "        p = self.players[begin:end] if self.players is not None else None\n",
    "        r = self.rusher[begin:end] if self.rusher is not None else None\n",
    "        m = self.meta[begin:end] if self.meta is not None else None\n",
    "        y = self.y[begin:end] if self.y is not None else None\n",
    "        yd = self.yardLine[begin:end].copy() if self.yardLine is not None else None\n",
    "        yr = self.year[begin:end].copy() if self.year is not None else None\n",
    "\n",
    "        return Data(p, r, m, y, yd, yr, self.player_cols, self.rusher_cols, self.meta_cols)\n",
    "\n",
    "    def _sample_by_mask(self, mask):\n",
    "        ## sub-sample observations, which will be used for data from year 2017 \n",
    "        mask_tensor = torch.from_numpy(mask)\n",
    "        p = self.players[mask_tensor] if self.players is not None else None\n",
    "        r = self.rusher[mask_tensor] if self.rusher is not None else None\n",
    "        m = self.meta[mask_tensor] if self.meta is not None else None\n",
    "        y = self.y[mask_tensor] if self.y is not None else None\n",
    "        yd = self.yardLine[mask].copy() if self.yardLine is not None else None\n",
    "        yr = self.year[mask].copy() if self.year is not None else None\n",
    "\n",
    "        return Data(p, r, m, y, yd, yr, self.player_cols, self.rusher_cols, self.meta_cols)\n",
    "\n",
    "    def downsample_2017(self, dropout_rate: float):\n",
    "        assert 0 <= dropout_rate <= 1.0\n",
    "        print(f\"down sample for games in 2017 with rate: {dropout_rate}\")\n",
    "        dropout = np.random.choice([True, False], size=len(self.year), p=[1 - dropout_rate, dropout_rate])\n",
    "        mask = (self.year != 2017) | dropout\n",
    "        return self._sample_by_mask(mask)\n",
    "\n",
    "    def shuffled(self):\n",
    "        print(f\"start shuffled\")\n",
    "        indices = np.random.permutation(self.players.shape[0])\n",
    "        p = np.take(self.players, indices, axis=0)\n",
    "        r = np.take(self.rusher, indices, axis=0) if self.rusher is not None else None\n",
    "        m = np.take(self.meta, indices, axis=0) if self.meta is not None else None\n",
    "        y = np.take(self.y, indices, axis=0) if self.y is not None else None\n",
    "        yd = np.take(self.yardLine, indices, axis=0).copy() if self.yardLine is not None else None\n",
    "        yr = np.take(self.year, indices, axis=0).copy() if self.year is not None else None\n",
    "        return Data(p, r, m, y, yd, yr, self.player_cols, self.rusher_cols, self.meta_cols)\n",
    "\n",
    "    def random_split(self, p: float):\n",
    "        mask = np.random.choice([True, False], p=[p, 1 - p], size=self.meta.size(0))\n",
    "        d1 = self._sample_by_mask(mask)\n",
    "        d2 = self._sample_by_mask(~mask)\n",
    "        return d1, d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  data from source files and split data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Load & Validation Split\n",
    "#######################################################################################################################\n",
    "\n",
    "def load_data_nn_test(test_df: pd.DataFrame, scaler: StandardScaler, scaler_meta: StandardScaler) -> Data:\n",
    "    play, players = prep(test_df)\n",
    "    players_nn, rusher_nn, meta_nn, _, _ = prep_players_nn(play, players, scaler, scaler_meta)\n",
    "    X = players_nn.values.astype(np.float32).reshape((-1, 22, len(players_nn.columns)))\n",
    "    Xr = rusher_nn.values.astype(np.float32)\n",
    "    Xm = meta_nn.values.astype(np.float32)\n",
    "    year = play['Year'].values.copy()\n",
    "\n",
    "    return Data(torch.from_numpy(X), torch.from_numpy(Xr), torch.from_numpy(Xm),\n",
    "                None, play['YardLine_std'].values.copy(), year,\n",
    "                list(players_nn.columns), list(rusher_nn.columns), list(meta_nn.columns))\n",
    "\n",
    "\n",
    "def load_data_nn(n_plays_sample=None,\n",
    "                 nfolds=None, nidx=None,\n",
    "                 skiprows=None,\n",
    "                 game_set=None) -> Tuple[Data, Data, StandardScaler, StandardScaler]:\n",
    "    print(f\"load and prepare training and validation data\")\n",
    "    print(f\"parameters to read_csv: nobs to read: {n_plays_sample}, nfolds:{nfolds}, skiprows{skiprows}\")\n",
    "    print(f\"game_set for splitting data into train and validation:{game_set}\")\n",
    "    \n",
    "    \n",
    "    print(f\"---start loading data from source file (train.csv) in load_data_nn()--\")\n",
    "    train = pd.read_csv('./input/train.csv', nrows=n_plays_sample, skiprows=skiprows)\n",
    "    \n",
    "    \n",
    "    play, players = prep(train)\n",
    "    \n",
    "    players_nn, rusher_nn, meta_nn, scaler, scaler_meta = prep_players_nn(play, players, None, None)\n",
    "\n",
    "    assert len(players_nn) == len(rusher_nn) * 22\n",
    "    assert len(rusher_nn) == len(meta_nn)\n",
    "    assert len(rusher_nn) == len(play)\n",
    "    \n",
    "    play_ids = np.array(players_nn.index[np.arange(0, len(players_nn), 22)])\n",
    "    game_ids = np.array(players['GameId'].values[np.arange(0, len(players_nn), 22)])\n",
    "\n",
    "    assert len(play_ids) == len(game_ids)\n",
    "\n",
    "    if nfolds is not None:\n",
    "        print(f\"prepare data into {nfolds} folds\")\n",
    "        assert nidx is not None\n",
    "        game_ids_valid = NFL_group_split(train, nfolds, nidx)\n",
    "    else:\n",
    "        if game_set is None:\n",
    "            print(f\"game set is None, and will use last 5 games for each team as validation\")\n",
    "        else:\n",
    "            print(f\"use game set to split data data:{game_set}\")      \n",
    "        game_ids_valid = NFL_validation_split(train, game_set)\n",
    "\n",
    "    X = players_nn.values.astype(np.float32).reshape((-1, 22, len(players_nn.columns)))\n",
    "    Xr = rusher_nn.values.astype(np.float32)\n",
    "    Xm = meta_nn.values.astype(np.float32)\n",
    "\n",
    "    y = np.vstack(play['Yards'].apply(return_delta).values).astype(np.float32)\n",
    "\n",
    "    yardLine = play['YardLine_std'].values\n",
    "    year = play['Year'].values\n",
    "\n",
    "    valid_mask = np.isin(game_ids, np.array(list(game_ids_valid)))\n",
    "\n",
    "    X_valid = torch.from_numpy(X[valid_mask])\n",
    "    Xr_valid = torch.from_numpy(Xr[valid_mask])\n",
    "    Xm_valid = torch.from_numpy(Xm[valid_mask])\n",
    "    y_valid = torch.from_numpy(y[valid_mask])\n",
    "    yd_valid = yardLine[valid_mask].copy()\n",
    "    yr_valid = year[valid_mask].copy()\n",
    "\n",
    "    X_train = torch.from_numpy(X[~valid_mask])\n",
    "    Xr_train = torch.from_numpy(Xr[~valid_mask])\n",
    "    Xm_train = torch.from_numpy(Xm[~valid_mask])\n",
    "    y_train = torch.from_numpy(y[~valid_mask])\n",
    "    yd_train = yardLine[~valid_mask].copy()\n",
    "    yr_train = year[~valid_mask].copy()\n",
    "\n",
    "    print(f'X_train: {X_train.shape}, {Xr_train.shape}, {Xm_train.shape}')\n",
    "    print(f'X_valid: {X_valid.shape}, {Xr_valid.shape}, {Xm_valid.shape}')\n",
    "    print(f'y_train: {y_train.shape}, y_valid: {y_valid.shape}')\n",
    "    print(f'players: {list(players_nn.columns)}')\n",
    "    print(f'rusher: {list(rusher_nn.columns)}')\n",
    "    print(f'meta: {list(meta_nn.columns)}')\n",
    "\n",
    "    dtrain = Data(X_train, Xr_train, Xm_train, y_train, yd_train, yr_train,\n",
    "                  list(players_nn.columns), list(rusher_nn.columns), list(meta_nn.columns))\n",
    "    dvalid = Data(X_valid, Xr_valid, Xm_valid, y_valid, yd_valid, yr_valid,\n",
    "                  list(players_nn.columns), list(rusher_nn.columns), list(meta_nn.columns))\n",
    "\n",
    "    assert len(X) == len(y)\n",
    "    return dtrain, dvalid, scaler, scaler_meta\n",
    "\n",
    "\n",
    "def return_delta(x):\n",
    "    temp = np.zeros(199)\n",
    "    temp[x + 99:] = 1\n",
    "    return temp\n",
    "\n",
    "\n",
    "def NFL_validation_split(df: pd.DataFrame, game_set=None):\n",
    "    games = df[['GameId', 'PossessionTeam']].drop_duplicates()\n",
    "\n",
    "    # Sort so the latest games are first and label the games with cumulative counter\n",
    "    games = games.sort_values(['PossessionTeam', 'GameId'], ascending=[True, False])\n",
    "    games['row_number'] = games.groupby(['PossessionTeam']).cumcount() + 1\n",
    "\n",
    "    # Use last 5 games for each team as validation. There will be overlap since two teams will have the same\n",
    "    # GameId\n",
    "    game_set = game_set or {1, 2, 3, 4, 5}\n",
    "\n",
    "    # Set of unique game ids\n",
    "    game_ids = set(games[games['row_number'].isin(game_set)]['GameId'].unique().tolist())\n",
    "\n",
    "    return game_ids\n",
    "\n",
    "\n",
    "def NFL_group_split(df: pd.DataFrame, nfolds: int, nidx: int):\n",
    "    kf = GroupKFold(nfolds)\n",
    "\n",
    "    train_idx, valid_idx = list(kf.split(df, groups=df['GameId']))[nidx]\n",
    "\n",
    "    return set(df['GameId'].iloc[valid_idx].unique())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def crps(y_pred, y_true):\n",
    "    loss = torch.mean((torch.cumsum(y_pred, dim=1) - y_true) ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to understand the data (shape, features,etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load and prepare training and validation data\n",
      "parameters to read_csv: nobs to read: None, nfolds:None, skiprowsNone\n",
      "game_set for splitting data into train and validation:None\n",
      "---start loading data from source file (train.csv) in load_data_nn()--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cas/DeepLearn/guilin/anaconda3/envs/pytorch_tf/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (47) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare features from source data with function: prep\n",
      "prepare player data with function: prep_players_nn\n",
      "game set is None, and will use last 5 games for each team as validation\n",
      "X_train: torch.Size([27274, 22, 15]), torch.Size([27274, 7]), torch.Size([27274, 2])\n",
      "X_valid: torch.Size([3733, 22, 15]), torch.Size([3733, 7]), torch.Size([3733, 2])\n",
      "y_train: torch.Size([27274, 199]), y_valid: torch.Size([3733, 199])\n",
      "players: ['X_std', 'Y_std', 'S', 'A', 'Dis', 'PlayerWeight', 'IsBallCarrier', 'IsOffence', 'Dir_cos', 'Dir_sin', 'DX', 'DY', 'AngleFromRB', 'AngleFromRB2', 'AngleTan']\n",
      "rusher: ['X_std', 'Y_std', 'S', 'A', 'Dis', 'Dir_cos', 'Dir_sin']\n",
      "meta: ['YardLine_std', 'Distance']\n",
      "---Members of train_data:---\n",
      " dict_keys(['players', 'rusher', 'meta', 'y', 'yardLine', 'year', 'player_cols', 'rusher_cols', 'meta_cols'])\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, scaler, scaler_meta = load_data_nn(None)\n",
    "print(f\"---Members of train_data:---\\n {train_data.__dict__.keys()}\")\n",
    "## dir(train_data) ## list available functions and members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle and downsample data for training\n",
      "start shuffled\n",
      "0.4\n",
      "down sample for games in 2017 with rate: 0.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"shuffle and downsample data for training\")\n",
    "data = train_data.shuffled()\n",
    "downsample_2017 = 0.4\n",
    "if downsample_2017 > 0.0:\n",
    "    print(downsample_2017)\n",
    "    data1 = data.downsample_2017(downsample_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a BERT model:  Encoder from transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# The Model\n",
    "#######################################################################################################################\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ninp: int, nemb: int = 1, nhead: int = 1, nhid: int = 32, nlayers: int = 4, nfinal: int = 1024,\n",
    "                 dropout_encoder: float = 0.1, dropout_embed: float = 0.0, dropout_classifier: float = 0.0,\n",
    "                 n_class: int = 199, ninp_rusher: int = 16, pre_LN: bool = False, rusher_emb: int = 32,\n",
    "                 n_emb_layers: int = 2,\n",
    "                 ninp_meta: int = 8, meta_emb: int = 32, gauss_noise: float = 0.0,\n",
    "                 gauss_xy_noise: float = 0.0,\n",
    "                 n_fin_layers: int = 3, dropout_attn: float = 0):\n",
    "        \"\"\"        \n",
    "         :param ninp: Number of input dimensions(feature dimension per player): 15\n",
    "         :param nemb: Embedding layer dimension\n",
    "         :param nhead: number of multi-head attention heads\n",
    "         :param nhid: hidden dimension of FeedForward(FFN) in transformer\n",
    "         :param nlayers: number of transformer-encoder layers\n",
    "         :param nfinal: Dimension of Linear layer after readout\n",
    "         :param dropout_encoder: Self-Attention, dropout in Encoder\n",
    "         :param dropout_embed: Embedding layer dropout\n",
    "         :param dropout_classifier: dropout after readout\n",
    "         :param n_class:\n",
    "         :param ninp_rusher: Rusher feature dimension\n",
    "         :param pre_LN: Whether to set Layer-Normalization placement method to pre-LN\n",
    "         :param rusher_emb: Rusher's embedding dimension        \n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        #encoder_layers = TransformerEncoderLayer(nemb, nhead, nhid, dropout_encoder, pre_LN=pre_LN, dropout_attn=dropout_attn)\n",
    "        #self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.config = BertConfig( \n",
    "            3, # no. vocaburary, no use\n",
    "            hidden_size=nemb,\n",
    "            num_hidden_layers=nlayers,\n",
    "            num_attention_heads=nhead,\n",
    "            intermediate_size=nhid,\n",
    "            hidden_dropout_prob=dropout_encoder,\n",
    "            attention_probs_dropout_prob=dropout_attn,\n",
    "            max_position_embeddings=22,\n",
    "            type_vocab_size=1 # \n",
    "        )\n",
    "\n",
    "        print(f\"bert encoder config:{self.config}\")\n",
    "        self.transformer_encoder  = BertEncoder(self.config)         \n",
    "        \n",
    "        #self.n_emb_layers = n_emb_layers\n",
    "        ## input shape for Conv1d : batch x input channels x seq len\n",
    "        ## output from Cov1d: batch x input channels x seq len, where the output seq len is decided by kernel size and stride\n",
    "        self.conv1 = nn.Conv1d(in_channels=ninp, out_channels=nemb, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=nemb, out_channels=nemb, kernel_size=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=nemb, out_channels=nemb, kernel_size=1)\n",
    "\n",
    "        self.relu1 = nn.PReLU()\n",
    "        self.relu2 = nn.PReLU()\n",
    "        self.relu3 = nn.PReLU()\n",
    "        self.relu4 = nn.PReLU()\n",
    "        self.relu5 = nn.ReLU()\n",
    "        if dropout_embed > 0:\n",
    "            self.dropout1 = nn.Dropout(dropout_embed)\n",
    "        else:\n",
    "            self.dropout1 = None\n",
    "\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=22)  # nn.MaxPool1d(kernel_size=22)\n",
    "\n",
    "        assert n_fin_layers == 3\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(dropout_classifier),\n",
    "            nn.Linear(nemb + rusher_emb + meta_emb, nfinal),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_classifier),\n",
    "            nn.Linear(nfinal, nfinal),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_classifier),\n",
    "            nn.Linear(nfinal, n_class)\n",
    "        )\n",
    "\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "        self.linear2 = nn.Linear(ninp_rusher, rusher_emb)\n",
    "        self.gauss_noise = gauss_noise\n",
    "        self.gauss_xy_noise = gauss_xy_noise\n",
    "        if meta_emb > 0:\n",
    "            self.linear3 = nn.Linear(ninp_meta, meta_emb)\n",
    "        else:\n",
    "            self.linear3 = None\n",
    "\n",
    "    def forward(self, x: Data, with_clip: bool = True):\n",
    "        # src: [Batch x Players(22) x Player Vector]\n",
    "        src = x.players\n",
    "        src_rusher = x.rusher\n",
    "        \n",
    "        \n",
    "        # src: [Batch x Player Vector x Players(22)] after permuation\n",
    "        src = src.permute([0, 2, 1])\n",
    "\n",
    "        if self.training:\n",
    "            # gaussian augmentation on training data\n",
    "            if self.gauss_noise > 0.0:\n",
    "                noise = torch.randn_like(src) * self.gauss_noise\n",
    "                src = src + noise\n",
    "                src = src.to(device) ## to device\n",
    "\n",
    "            if self.gauss_xy_noise > 0.0:\n",
    "                # dx = torch.randn(src.size(0)) * self.gauss_xy_noise\n",
    "                dy = torch.randn(src.size(0)) * self.gauss_xy_noise\n",
    "                dy = dy.to(device)\n",
    "                # Batch x 1 x 22\n",
    "                # src[:, 0, :] += dx.reshape(src.size(0), 1).expand(src.size(0), src.size(2))\n",
    "                src[:, 1, :] += dy.reshape(src.size(0), 1).expand(src.size(0), src.size(2))\n",
    "\n",
    "                # src_rusher[:, 0] += dx\n",
    "                src_rusher[:, 1] += dy\n",
    "\n",
    "        # src: [Batch x Player Vector(embedded, 4*inp) x Players(22)]\n",
    "        # conv1d expect: [batch_size, in_channels, len]\n",
    "        # With kernel size 1, no interaction between players\n",
    "        src = self.relu1(self.conv1(src))\n",
    "        src = self.dropout1(src)\n",
    "        src = self.relu2(self.conv2(src))\n",
    "        src = self.dropout1(src)\n",
    "        src = self.relu3(self.conv3(src))\n",
    "\n",
    "        src = src.permute([2, 0, 1])\n",
    "\n",
    "        # output: [Players(22) x Batch x Transformed Player Vector]\n",
    "        head_mask = [None] * self.config.num_hidden_layers\n",
    "        #extended_attention_mask, head_mask=head_mask\n",
    "        # input shape for encoder: seq len x batch x embedding len\n",
    "        output = self.transformer_encoder(src, attention_mask=None, head_mask=head_mask)[0]\n",
    "\n",
    "        # output: [Batch x Transformed Player Vector x Players(22)]\n",
    "        output = output.permute([1, 2, 0])\n",
    "\n",
    "        # output: [Batch x Transformed Player Vector]\n",
    "        output = torch.squeeze(self.avgpool(output), dim=2)\n",
    "\n",
    "        if self.linear3 is not None:\n",
    "            output = torch.cat((output, self.relu4(self.linear2(src_rusher)), self.relu5(self.linear3(x.meta))), dim=1)\n",
    "        else:\n",
    "            output = torch.cat((output, self.relu4(self.linear2(src_rusher))), dim=1)\n",
    "\n",
    "        # output: [Batch x n_class]\n",
    "        output = self.linear(output)\n",
    "        output = self.activation(output)\n",
    "\n",
    "        if not self.training and x.yardLine is not None:\n",
    "            \n",
    "            output = torch.cumsum(output, dim=1).cpu().numpy()\n",
    "\n",
    "            output = np.clip(output, 0.0, 1.0)\n",
    "\n",
    "            # mask\n",
    "            if with_clip:\n",
    "                left = 99 - x.yardLine\n",
    "                right = 199 - x.yardLine\n",
    "                for k in range(len(output)):\n",
    "                    output[k, :left[k] + 1] = 0.0\n",
    "                    output[k, right[k]:] = 1.0\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=16, device='cuda:0', downsample_2017=0.4, dropout_attn=0.35, dropout_classifier=0.3, dropout_embed=0.15, dropout_encoder=0.0, epochs=50, gamma=0.976, gauss_noise=0.15, gauss_xy_noise=0.1, log_filename='encoder_model_log.txt', lr=0.0001, meta_emb=8, model_name='transformer_dsbowl', n_emb_layers=3, n_fin_layers=3, nemb=128, nfinal=512, nhead=1, nhid=96, ninp=15, nlayers=4, save_dir='.', weight_decay=1e-06)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--ninp', default=15)\n",
    "parser.add_argument('--nemb', default=128)\n",
    "parser.add_argument('--nhead', default=1)\n",
    "parser.add_argument('--nhid', default=96)  # number of hidden units in attention\n",
    "parser.add_argument('--nlayers', default = 4)  # number of transformers stacked\n",
    "parser.add_argument('--nfinal', default = 512)  # number of hidden units in final layers\n",
    "\n",
    "parser.add_argument('--gamma', default=0.976)\n",
    "parser.add_argument('--dropout_encoder', default=0.0)\n",
    "parser.add_argument('--n_emb_layers', default=3)\n",
    "parser.add_argument('--dropout_classifier', default=0.3)\n",
    "parser.add_argument('--dropout_embed', default=0.15)\n",
    "parser.add_argument('--dropout_attn', default=0.35)\n",
    "\n",
    "\n",
    "parser.add_argument('--meta_emb', default=8)\n",
    "parser.add_argument('--downsample_2017', default=0.4)\n",
    "parser.add_argument('--gauss_noise', default=0.15)\n",
    "parser.add_argument('--gauss_xy_noise', default=0.1)\n",
    "parser.add_argument('--n_fin_layers', default=3)\n",
    "parser.add_argument('--model_name', default='transformer_dsbowl')\n",
    "parser.add_argument('--epochs', type=int, default=50)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-6)\n",
    "parser.add_argument('--device', default='cuda:0')\n",
    "parser.add_argument('--save_dir', default='.') ## model saved at the current directory\n",
    "parser.add_argument('--log_filename', default='encoder_model_log.txt')\n",
    "args = parser.parse_args(\"\")  \n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model specifications from params: {'ninp': 15, 'nemb': 128, 'nhead': 1, 'nhid': 96, 'nlayers': 4, 'nfinal': 512, 'ninp_rusher': 7, 'pre_LN': True, 'dropout_encoder': 0.0, 'dropout_embed': 0.15, 'dropout_classifier': 0.3, 'n_emb_layers': 3, 'ninp_meta': 2, 'meta_emb': 8, 'gauss_noise': 0.15, 'gauss_xy_noise': 0.1, 'n_fin_layers': 3, 'dropout_attn': 0.35}\n",
      "bert encoder config:BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.35,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 96,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 22,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 3\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### prepare model specification from parsed options\n",
    "params = {\n",
    "    'ninp': args.ninp, ## from train_data.players.shape[2],\n",
    "    'nemb': args.nemb,\n",
    "    'nhead': args.nhead,\n",
    "    'nhid': args.nhid,\n",
    "    'nlayers': args.nlayers,\n",
    "    'nfinal': args.nfinal,\n",
    "    'ninp_rusher': train_data.rusher.shape[1],\n",
    "    'pre_LN': True,\n",
    "    'dropout_encoder': args.dropout_encoder,\n",
    "    'dropout_embed': args.dropout_embed,\n",
    "    'dropout_classifier': args.dropout_classifier,\n",
    "    'n_emb_layers': args.n_emb_layers,\n",
    "    'ninp_meta': train_data.meta.shape[1],\n",
    "    'meta_emb': args.meta_emb,\n",
    "    'gauss_noise': args.gauss_noise,\n",
    "    'gauss_xy_noise': args.gauss_xy_noise,\n",
    "    'n_fin_layers': args.n_fin_layers,\n",
    "    'dropout_attn': args.dropout_attn\n",
    "    \n",
    "}\n",
    "\n",
    "print(f\"model specifications from params: {params}\")\n",
    "##NOTE: pass parameters with key value pairs: **params\n",
    "model = TransformerModel(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (transformer_encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=96, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=96, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=96, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=96, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=96, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=96, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=96, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=96, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv1): Conv1d(15, 128, kernel_size=(1,), stride=(1,))\n",
       "  (conv2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "  (conv3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "  (relu1): PReLU(num_parameters=1)\n",
       "  (relu2): PReLU(num_parameters=1)\n",
       "  (relu3): PReLU(num_parameters=1)\n",
       "  (relu4): PReLU(num_parameters=1)\n",
       "  (relu5): ReLU()\n",
       "  (dropout1): Dropout(p=0.15, inplace=False)\n",
       "  (avgpool): AvgPool1d(kernel_size=(22,), stride=(22,), padding=(0,))\n",
       "  (linear): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): Linear(in_features=168, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.3, inplace=False)\n",
       "    (7): Linear(in_features=512, out_features=199, bias=True)\n",
       "  )\n",
       "  (activation): Softmax(dim=1)\n",
       "  (linear2): Linear(in_features=7, out_features=32, bias=True)\n",
       "  (linear3): Linear(in_features=2, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the number of model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of model parameters:852067\n"
     ]
    }
   ],
   "source": [
    "### model parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad,model.parameters())\n",
    "print(f\"number of model parameters:{sum([np.prod(p.size()) for p in model_parameters])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display first k layer's names and their shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shows the first 20 layers from model\n",
      "transformer_encoder.layer.0.attention.self.query.weight torch.Size([128, 128])\n",
      "transformer_encoder.layer.0.attention.self.query.bias torch.Size([128])\n",
      "transformer_encoder.layer.0.attention.self.key.weight torch.Size([128, 128])\n",
      "transformer_encoder.layer.0.attention.self.key.bias torch.Size([128])\n",
      "transformer_encoder.layer.0.attention.self.value.weight torch.Size([128, 128])\n",
      "transformer_encoder.layer.0.attention.self.value.bias torch.Size([128])\n",
      "transformer_encoder.layer.0.attention.output.dense.weight torch.Size([128, 128])\n",
      "transformer_encoder.layer.0.attention.output.dense.bias torch.Size([128])\n",
      "transformer_encoder.layer.0.attention.output.LayerNorm.weight torch.Size([128])\n",
      "transformer_encoder.layer.0.attention.output.LayerNorm.bias torch.Size([128])\n",
      "transformer_encoder.layer.0.intermediate.dense.weight torch.Size([96, 128])\n",
      "transformer_encoder.layer.0.intermediate.dense.bias torch.Size([96])\n",
      "transformer_encoder.layer.0.output.dense.weight torch.Size([128, 96])\n",
      "transformer_encoder.layer.0.output.dense.bias torch.Size([128])\n",
      "transformer_encoder.layer.0.output.LayerNorm.weight torch.Size([128])\n",
      "transformer_encoder.layer.0.output.LayerNorm.bias torch.Size([128])\n",
      "transformer_encoder.layer.1.attention.self.query.weight torch.Size([128, 128])\n",
      "transformer_encoder.layer.1.attention.self.query.bias torch.Size([128])\n",
      "transformer_encoder.layer.1.attention.self.key.weight torch.Size([128, 128])\n",
      "transformer_encoder.layer.1.attention.self.key.bias torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "## model structure and its weights\n",
    "nLayers = 20\n",
    "print(f\"Shows the first {nLayers} layers from model\")\n",
    "i=0\n",
    "for name, param in model.named_parameters():\n",
    "    if i<nLayers:\n",
    "        if param.requires_grad:\n",
    "            print (name, param.shape)\n",
    "            i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training\n",
    "- evaluation metric\n",
    "- repeat/loop train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model: nn.Module, data: Data, with_clip: bool = True, device: torch.device =None):\n",
    "    #print(f\"in evalueate, device:{device}\")\n",
    "    eval_model.eval()  # Turn on the evaluation mode\n",
    "    assert data.y is not None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_actual = data.y.numpy()\n",
    "        '''\n",
    "        data.players = data.players.to(device)\n",
    "        data.rusher = data.rusher.to(device)\n",
    "        data.meta = data.meta.to(device)\n",
    "        '''\n",
    "        data = data_to_device(data, device)\n",
    "        y_predicted = eval_model(data, with_clip)\n",
    "\n",
    "        loss = np.sum((y_predicted - y_actual) ** 2) / (199 * len(y_predicted)) ## 199 predictions for one sample\n",
    "\n",
    "    return loss\n",
    "## move data to device for model evaluation\n",
    "def data_to_device(data, device):\n",
    "    ## data on device for model evaluation\n",
    "    data.players = data.players.to(device)\n",
    "    data.rusher = data.rusher.to(device)\n",
    "    data.meta = data.meta.to(device)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Training & Pseudo-Labeling\n",
    "#######################################################################################################################\n",
    "\n",
    "def train_model(model: nn.Module, scheduler, batch_size: int,\n",
    "                train_data: Data, valid_data: Data, writer,\n",
    "                epochs:int,\n",
    "                downsample_2017: float,\n",
    "                calc_train_loss: bool = True,\n",
    "                params: Dict = None,\n",
    "                device: torch.device =None):\n",
    "    print(f\"in train_model, device:{device}\")\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Keep number of data in 1 epoch same between 1st/2nd stage\n",
    "    std_batch_length = 12000 // batch_size + 1\n",
    "    n_total_batch = 0\n",
    "    epoch = 1\n",
    "    epoch_start_time = time.time()\n",
    "    ensemble = None\n",
    "    print(f\"epochs in train_model:{epochs}\")\n",
    "    model.eval()\n",
    "\n",
    "    snapshots = SnapShots(torelance=1.007, interval=3)\n",
    "\n",
    "    snapshots.add(model, 0, evaluate(model, valid_data,device=device))\n",
    "\n",
    "    model.train()  # Turn on the train mode\n",
    "\n",
    "    while epoch < epochs:\n",
    "        print(f\"start training at epoch: {epoch} and shuffled\")\n",
    "        data = train_data.shuffled()\n",
    "        if downsample_2017 > 0.0:\n",
    "            data = data.downsample_2017(downsample_2017)\n",
    "        for i in range(0, data.len() - 1, batch_size):\n",
    "            model.train()  # Turn on the train mode\n",
    "            iend = min(i + batch_size, data.len())\n",
    "            batch_data = data.slice(i, iend)\n",
    "            optimizer.zero_grad()\n",
    "            batch_data = data_to_device(batch_data, device)\n",
    "            output = model(batch_data)\n",
    "\n",
    "            loss = crps(output, batch_data.y.to(device))\n",
    "            #print(f\"loss type: {loss.is_cuda}\")            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            n_total_batch += 1\n",
    "\n",
    "            if n_total_batch > std_batch_length:\n",
    "                model.eval()\n",
    "                \n",
    "                if calc_train_loss and epoch % 1 == 0:\n",
    "                    #train_loss = evaluate(model, data, device=device)\n",
    "                    train_loss = loss.item() ## from one batch only\n",
    "                else:\n",
    "                    train_loss = -1\n",
    "\n",
    "                if epoch % 1 == 0 or epoch > 30:\n",
    "                    ## data to device will be done in evaluate\n",
    "                    ## IMPORTANT: due to limit memory, evaluate in batch mode on GPU\n",
    "                    val_loss = 0\n",
    "                    for i in range(0, valid_data.len() - 1, 100):\n",
    "                        iend = min(i + 100, valid_data.len())\n",
    "                        batch_valid_data = valid_data.slice(i, iend)                  \n",
    "                        val_loss_batch = evaluate(model, batch_valid_data, device=device)\n",
    "                        val_loss += val_loss_batch*(iend-i+1)\n",
    "                        \n",
    "                    val_loss = val_loss/valid_data.len()    \n",
    "                    snapshots.add(model, epoch, val_loss/valid_data.len())\n",
    "                else:\n",
    "                    val_loss = -1\n",
    "\n",
    "                print('| end of epoch {:3d} | lr: {:2.5f} | time: {:5.2f}s | train loss {:5.6f} | '\n",
    "                      'valid loss {:5.9f}'.format(epoch, scheduler.get_lr()[0], (time.time() - epoch_start_time),\n",
    "                                                  train_loss, val_loss))\n",
    "                #scheduler.step()\n",
    "                if writer is not None:\n",
    "                    writer.writerow([epoch, scheduler.get_lr()[0], train_loss, val_loss])\n",
    "                    f.flush()\n",
    "\n",
    "                n_total_batch = 0\n",
    "                epoch += 1\n",
    "                epoch_start_time = time.time()\n",
    "                model.train()  # Turn on the train mode\n",
    "                scheduler.step()\n",
    "                #print(f\"after step, lr:{scheduler.get_lr()[0]}\")\n",
    "                if epoch in [40]:\n",
    "                    batch_size *= 2\n",
    "                    std_batch_length = 12000 // batch_size + 1\n",
    "\n",
    "    # reload best model\n",
    "    model = snapshots.load_best_single_model(model)\n",
    "    eval_single = evaluate(model, valid_data)\n",
    "\n",
    "    try:\n",
    "        ensemble = snapshots.load_ensemble_model(TransformerModel, params)\n",
    "        eval_ensemble = evaluate(ensemble, valid_data)\n",
    "        print(f'final loss: {eval_single:.8f} (single) / {eval_ensemble:.8f} ({len(ensemble.models)} models)')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return ensemble if ensemble is not None else model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Ensemble through Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "## SnapShot & Ensemble\n",
    "#######################################################################################################################\n",
    "\n",
    "class EnsembleModel(object):\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "    def train(self):\n",
    "        for m in self.models:\n",
    "            m.train()\n",
    "\n",
    "    def eval(self):\n",
    "        for m in self.models:\n",
    "            m.eval()\n",
    "\n",
    "    def __call__(self, *input, **kwargs):\n",
    "        assert len(self.models) >= 1\n",
    "\n",
    "        base = self.models[0](*input, **kwargs)\n",
    "\n",
    "        for m in self.models[1:]:\n",
    "            base += m(*input, **kwargs)\n",
    "\n",
    "        return base / len(self.models)\n",
    "\n",
    "\n",
    "class SnapShot(object):\n",
    "    def __init__(self, model: nn.Module, epoch: int, loss: float):\n",
    "        self.state = copy.deepcopy(model.state_dict())\n",
    "        self.epoch = epoch\n",
    "        self.loss = loss\n",
    "        torch.save(self.state, f'snapshot_{epoch}_{loss}')\n",
    "\n",
    "\n",
    "class SnapShots(object):\n",
    "    def __init__(self, interval: int = 3, torelance: float = 1.01, verbose: bool = True):\n",
    "        self.best_model = None  # type: Optional[SnapShot]\n",
    "        self.snap_shots = []  # type: List[SnapShot]\n",
    "        self.best_val_loss = 1.0\n",
    "        self.interval = interval\n",
    "        self.torelance = torelance\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def add(self, model: nn.Module, epoch: int, loss: float):\n",
    "        if loss < self.best_val_loss:\n",
    "            if self.verbose:\n",
    "                print(f'best model is updated. epoch{epoch}, loss={loss:.7f}')\n",
    "            self.best_model = SnapShot(model, epoch, loss)\n",
    "            self.best_val_loss = loss\n",
    "        if epoch % self.interval == 0:\n",
    "            if self.verbose:\n",
    "                print(f'Add snapshot. epoch{epoch}, loss={loss:.7f}')\n",
    "            self.snap_shots.append(SnapShot(model, epoch, loss))\n",
    "\n",
    "    def load_best_single_model(self, model: nn.Module):\n",
    "        if self.best_model is not None:\n",
    "            model.load_state_dict(self.best_model.state)\n",
    "        return model\n",
    "\n",
    "    def load_ensemble_model(self, cls: Type, params: Dict, max_models: int = 5):\n",
    "        model = EnsembleModel()\n",
    "        best = cls(**params)\n",
    "        self.load_best_single_model(best)\n",
    "        model.add_model(best)\n",
    "\n",
    "        candidates = []  # Tuple[int, float]\n",
    "\n",
    "        for i, s in enumerate(self.snap_shots):\n",
    "            if s.loss > self.torelance * self.best_model.loss:\n",
    "                continue\n",
    "            if s.epoch == self.best_model.epoch:\n",
    "                continue\n",
    "\n",
    "            candidates.append((i, s.loss))\n",
    "\n",
    "        # Collect top-n models from snapshot\n",
    "        candidates = sorted(candidates, key = lambda x: x[1])\n",
    "        if len(candidates) > max_models - 1:\n",
    "            candidates = candidates[:max_models - 1]\n",
    "        for i, _ in candidates:\n",
    "            s = self.snap_shots[i]\n",
    "            sub = cls(**params)\n",
    "            sub.load_state_dict(s.state)\n",
    "            model.add_model(sub)\n",
    "            if self.verbose:\n",
    "                print(f'add {s.epoch}-th epoch to ensemble (loss:{s.loss:.7f})')\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=16, device='cuda:0', downsample_2017=0.4, dropout_attn=0.35, dropout_classifier=0.3, dropout_embed=0.15, dropout_encoder=0.0, epochs=50, gamma=0.976, gauss_noise=0.15, gauss_xy_noise=0.1, log_filename='encoder_model_log.txt', lr=0.0001, meta_emb=8, model_name='transformer_dsbowl', n_emb_layers=3, n_fin_layers=3, nemb=128, nfinal=512, nhead=1, nhid=96, ninp=15, nlayers=4, save_dir='.', weight_decay=1e-06)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to train transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 8.82 µs\n",
      "single mode training\n",
      "args:Namespace(batch_size=16, device='cuda:0', downsample_2017=0.4, dropout_attn=0.35, dropout_classifier=0.3, dropout_embed=0.15, dropout_encoder=0.0, epochs=50, gamma=0.976, gauss_noise=0.15, gauss_xy_noise=0.1, log_filename='encoder_model_log.txt', lr=0.0001, meta_emb=8, model_name='transformer_dsbowl', n_emb_layers=3, n_fin_layers=3, nemb=128, nfinal=512, nhead=1, nhid=96, ninp=15, nlayers=4, save_dir='.', weight_decay=1e-06)\n",
      "---train on device:cuda:0---\n",
      "load and prepare training and validation data\n",
      "parameters to read_csv: nobs to read: None, nfolds:None, skiprowsNone\n",
      "game_set for splitting data into train and validation:None\n",
      "---start loading data from source file (train.csv) in load_data_nn()--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cas/DeepLearn/guilin/anaconda3/envs/pytorch_tf/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (47) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare features from source data with function: prep\n",
      "prepare player data with function: prep_players_nn\n",
      "game set is None, and will use last 5 games for each team as validation\n",
      "X_train: torch.Size([27274, 22, 15]), torch.Size([27274, 7]), torch.Size([27274, 2])\n",
      "X_valid: torch.Size([3733, 22, 15]), torch.Size([3733, 7]), torch.Size([3733, 2])\n",
      "y_train: torch.Size([27274, 199]), y_valid: torch.Size([3733, 199])\n",
      "players: ['X_std', 'Y_std', 'S', 'A', 'Dis', 'PlayerWeight', 'IsBallCarrier', 'IsOffence', 'Dir_cos', 'Dir_sin', 'DX', 'DY', 'AngleFromRB', 'AngleFromRB2', 'AngleTan']\n",
      "rusher: ['X_std', 'Y_std', 'S', 'A', 'Dis', 'Dir_cos', 'Dir_sin']\n",
      "meta: ['YardLine_std', 'Distance']\n",
      "bert encoder config:BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.35,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 96,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 22,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 3\n",
      "}\n",
      "\n",
      "in train_model, device:cuda:0\n",
      "epochs in train_model:3\n",
      "best model is updated. epoch0, loss=0.0664664\n",
      "Add snapshot. epoch0, loss=0.0664664\n",
      "start training at epoch: 1 and shuffled\n",
      "start shuffled\n",
      "down sample for games in 2017 with rate: 0.4\n",
      "best model is updated. epoch1, loss=0.0000036\n",
      "| end of epoch   1 | lr: 0.00010 | time: 19.35s | train loss 0.023324 | valid loss 0.013585863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cas/DeepLearn/guilin/anaconda3/envs/pytorch_tf/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:350: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training at epoch: 2 and shuffled\n",
      "start shuffled\n",
      "down sample for games in 2017 with rate: 0.4\n",
      "best model is updated. epoch2, loss=0.0000036\n",
      "| end of epoch   2 | lr: 0.00010 | time: 18.83s | train loss 0.008589 | valid loss 0.013412119\n",
      "best model is updated. epoch3, loss=0.0000036\n",
      "Add snapshot. epoch3, loss=0.0000036\n",
      "| end of epoch   3 | lr: 0.00009 | time: 17.77s | train loss 0.007942 | valid loss 0.013256483\n",
      "bert encoder config:BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.35,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 96,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 22,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 3\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "### debug purpose\n",
    "print(f\"single mode training\")\n",
    " \n",
    "mode = 'single'\n",
    "n_holdout = 0\n",
    "n_train = None\n",
    "print(f\"args:{args}\")\n",
    "#device='cuda:0'\n",
    "#device ='cpu'\n",
    "device = torch.device(args.device)\n",
    "## For debug purpose, use a small number of epochs\n",
    "args.epochs = 3\n",
    "print(f\"---train on device:{device}---\")\n",
    "train_data, valid_data, scaler, scaler_meta = load_data_nn(None) \n",
    "model = TransformerModel(**params)\n",
    "#model = TransformerModel(**params)\n",
    "model.to(device)\n",
    "f = open(args.log_filename + '.csv', 'w+', newline='')\n",
    "writer = csv.writer(f)\n",
    "writer.writerow(train_data.player_cols)\n",
    "writer.writerow(train_data.rusher_cols)\n",
    "writer.writerow(['epoch', 'lr', 'train_loss', 'valid_loss'])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)  \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=args.gamma)\n",
    "\n",
    "model = train_model(model, scheduler, args.batch_size, train_data, valid_data, writer, \n",
    "                    args.epochs, downsample_2017, params=params, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Train models in different modes:\n",
    " - Single mode\n",
    " - Ensemble mode\n",
    " - Grid mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_in_mode(params, mode):\n",
    "    if mode == 'grid':\n",
    "        print(f\"grid mode training\")\n",
    "        train_data, valid_data, scaler, scaler_meta = load_data_nn(None)\n",
    "        for dropout_embed in [0.05, 0.1, 0.15]:\n",
    "            for dropout_encoder in [0.2, 0.3, 0.4]:\n",
    "                log_f = f'{log_filename}_embed{dropout_embed}_encoder{dropout_encoder}.csv'\n",
    "                params['dropout_encoder'] = dropout_encoder\n",
    "                params['dropout_embed'] = dropout_embed\n",
    "                model = TransformerModel(**params)\n",
    "                model.to(args.device)\n",
    "                f = open(log_f, 'w+', newline='')\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([dropout_embed, dropout_classifier])\n",
    "                writer.writerow(['epoch', 'lr', 'train_loss', 'valid_loss'])\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=args.gamma)\n",
    "\n",
    "                model = train_model(model, scheduler, args.batch_size, train_data, valid_data, \n",
    "                                    writer,  args.epochs, downsample_2017, params=params,device=args.device)\n",
    "\n",
    "    elif mode == 'ensemble':\n",
    "        print(f\"ensemble training\")\n",
    "        models = EnsembleModel()\n",
    "        game_sets = [\n",
    "            {1, 3, 5, 7, 9},\n",
    "            {2, 4, 6, 8, 10}\n",
    "        ]\n",
    "\n",
    "        for i in range(len(game_sets)):\n",
    "            model = TransformerModel(**params)\n",
    "            model.to(args.device)\n",
    "            train_data, valid_data, scaler, scaler_meta = load_data_nn(None, nfolds=8, nidx=i) #load_data_nn(None, game_set=game_sets[i])\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)    \n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=args.gamma)\n",
    "\n",
    "            model = train_model(model, scheduler, args.batch_size, train_data, valid_data, None, args.epochs, downsample_2017,\n",
    "                                calc_train_loss=True, params=param,device=args.device)\n",
    "            models.add_model(model)\n",
    "        model = models\n",
    "    else:\n",
    "        assert mode == 'single'\n",
    "        print(f\"single mode training\")\n",
    "        train_data, valid_data, scaler, scaler_meta = load_data_nn(None)        \n",
    "        model = TransformerModel(**params)\n",
    "        model.to(args.device)\n",
    "        f = open(log_filename + '.csv', 'w+', newline='')\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(train_data.player_cols)\n",
    "        writer.writerow(train_data.rusher_cols)\n",
    "        writer.writerow(['epoch', 'lr', 'train_loss', 'valid_loss'])\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)  \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=args.gamma) ## gamma decay factor of learning\n",
    "        \n",
    "        model = train_model(model, scheduler, args.batch_size, train_data, valid_data, writer, args.epochs, downsample_2017,\n",
    "                            params=params, device=args.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble training\n",
      "bert encoder config:BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.35,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 96,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 22,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 3\n",
      "}\n",
      "\n",
      "load and prepare training and validation data\n",
      "parameters to read_csv: nobs to read: None, nfolds:8, skiprowsNone\n",
      "game_set for splitting data into train and validation:None\n",
      "---start loading data from source file (train.csv) in load_data_nn()--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cas/DeepLearn/guilin/anaconda3/envs/pytorch_tf/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (47) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare features from source data with function: prep\n",
      "prepare player data with function: prep_players_nn\n",
      "prepare data into 8 folds\n",
      "X_train: torch.Size([27129, 22, 15]), torch.Size([27129, 7]), torch.Size([27129, 2])\n",
      "X_valid: torch.Size([3878, 22, 15]), torch.Size([3878, 7]), torch.Size([3878, 2])\n",
      "y_train: torch.Size([27129, 199]), y_valid: torch.Size([3878, 199])\n",
      "players: ['X_std', 'Y_std', 'S', 'A', 'Dis', 'PlayerWeight', 'IsBallCarrier', 'IsOffence', 'Dir_cos', 'Dir_sin', 'DX', 'DY', 'AngleFromRB', 'AngleFromRB2', 'AngleTan']\n",
      "rusher: ['X_std', 'Y_std', 'S', 'A', 'Dis', 'Dir_cos', 'Dir_sin']\n",
      "meta: ['YardLine_std', 'Distance']\n",
      "in train_model, device:cuda:0\n",
      "epochs in train_model:10\n",
      "best model is updated. epoch0, loss=0.0659919\n",
      "Add snapshot. epoch0, loss=0.0659919\n",
      "start training at epoch: 1 and shuffled\n",
      "start shuffled\n",
      "down sample for games in 2017 with rate: 0.4\n",
      "best model is updated. epoch1, loss=0.0000172\n",
      "| end of epoch   1 | lr: 0.00010 | time: 19.30s | train loss 0.085103 | valid loss 0.066655046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cas/DeepLearn/guilin/anaconda3/envs/pytorch_tf/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:350: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "/cas/DeepLearn/guilin/anaconda3/envs/pytorch_tf/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training at epoch: 2 and shuffled\n",
      "start shuffled\n",
      "down sample for games in 2017 with rate: 0.4\n",
      "| end of epoch   2 | lr: 0.00010 | time: 19.26s | train loss 0.083864 | valid loss 0.066655046\n",
      "Add snapshot. epoch3, loss=0.0000172\n",
      "| end of epoch   3 | lr: 0.00009 | time: 19.28s | train loss 0.085332 | valid loss 0.066655046\n",
      "start training at epoch: 4 and shuffled\n",
      "start shuffled\n",
      "down sample for games in 2017 with rate: 0.4\n",
      "| end of epoch   4 | lr: 0.00009 | time: 18.90s | train loss 0.092580 | valid loss 0.066655046\n",
      "| end of epoch   5 | lr: 0.00009 | time: 18.93s | train loss 0.085941 | valid loss 0.066655046\n",
      "start training at epoch: 6 and shuffled\n",
      "start shuffled\n",
      "down sample for games in 2017 with rate: 0.4\n",
      "Add snapshot. epoch6, loss=0.0000172\n",
      "| end of epoch   6 | lr: 0.00009 | time: 17.78s | train loss 0.084727 | valid loss 0.066655046\n"
     ]
    }
   ],
   "source": [
    "## choose different mode to train model\n",
    "mode = 'ensemble'\n",
    "args.epochs = 10\n",
    "log_filename = 'ensemble'\n",
    "no_decay = ['bias', '.norm']\n",
    "train_in_mode(params, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dataset(l: Data, r: Data):\n",
    "    p = torch.cat((l.players, r.players))\n",
    "    rs = torch.cat((l.rusher, r.rusher))\n",
    "    m = torch.cat((l.meta, r.meta))\n",
    "    y = torch.cat((l.y, r.y)) if l.y is not None else None\n",
    "    yd = np.concatenate([l.yardLine, r.yardLine]) if l.yardLine is not None else None\n",
    "    yr = np.concatenate([l.year, r.year]) if l.year is not None else None\n",
    "    print(p.size())\n",
    "    print(yd.shape)\n",
    "    return Data(p, rs, m, y, yd, yr, l.player_cols, l.rusher_cols, l.meta_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta(test_df, sigma_dir=1.0, sigma_y=1.0):\n",
    "    n_aug = 10\n",
    "    test_df_aug = pd.concat([test_df]*n_aug)\n",
    "    \n",
    "    test_df_aug['Dir'] += np.random.normal(0, sigma_dir, size=len(test_df_aug))\n",
    "\n",
    "    # yは共通で上げ下げ\n",
    "    test_df_aug['Y'] += np.repeat(np.random.normal(0, sigma_y, size=n_aug), 22)\n",
    "    test_df_aug['PlayId'] += np.repeat(np.arange(0, n_aug), 22)\n",
    "    \n",
    "    return test_df_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Turn on the evaluation mode\n",
    "n_prev = 0\n",
    "\n",
    "original_data = concat_dataset(train_data, valid_data)\n",
    "\n",
    "for (test_df, sample_prediction_df) in env.iter_test():\n",
    "    try:\n",
    "        test_df = tta(test_df)\n",
    "        X_test = load_data_nn_test(test_df, scaler, scaler_meta)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'### ERROR ### {e} / {test_df}')\n",
    "        # submit as-is if something happened\n",
    "        env.predict(sample_prediction_df)\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted = model(X_test).mean(axis=0)\n",
    "        sample_prediction_df.iloc[0,:] = np.squeeze(predicted)\n",
    "        env.predict(sample_prediction_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.write_submission_file()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
