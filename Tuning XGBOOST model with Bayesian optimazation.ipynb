{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example from Kaggle competition: Home Credit Default Risk\n",
    "- 1. How to create features and do feature selections\n",
    "- 2. How to tune xgboost using Bayesian optimization \n",
    "- refer to https://www.kaggle.com/c/home-credit-default-risk/\n",
    "- refer to https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forked from excellent kernel : https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features\n",
    "# From Kaggler : https://www.kaggle.com/jsaguiar\n",
    "# Just added a few features so I thought I had to make release it as well...\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing tasks with context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## timing tasks with contextmanager\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv('./input/application_train.csv', nrows= num_rows)\n",
    "    test_df = pd.read_csv('./input/application_test.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    docs = [_f for _f in df.columns if 'FLAG_DOC' in _f]\n",
    "    live = [_f for _f in df.columns if ('FLAG_' in _f) & ('FLAG_DOC' not in _f) & ('_FLAG_' not in _f)]\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "    inc_by_org = df[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n",
    "\n",
    "    df['NEW_CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "    df['NEW_CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    df['NEW_DOC_IND_AVG'] = df[docs].mean(axis=1)\n",
    "    df['NEW_DOC_IND_STD'] = df[docs].std(axis=1)\n",
    "    df['NEW_DOC_IND_KURT'] = df[docs].kurtosis(axis=1)\n",
    "    df['NEW_LIVE_IND_SUM'] = df[live].sum(axis=1)\n",
    "    df['NEW_LIVE_IND_STD'] = df[live].std(axis=1)\n",
    "    df['NEW_LIVE_IND_KURT'] = df[live].kurtosis(axis=1)\n",
    "    df['NEW_INC_PER_CHLD'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])\n",
    "    df['NEW_INC_BY_ORG'] = df['ORGANIZATION_TYPE'].map(inc_by_org)\n",
    "    df['NEW_EMPLOY_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['NEW_ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / (1 + df['AMT_INCOME_TOTAL'])\n",
    "    df['NEW_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    df['NEW_EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df['NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    df['NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n",
    "    df['NEW_CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "    df['NEW_CAR_TO_EMPLOY_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "    df['NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "    df['NEW_PHONE_TO_EMPLOY_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "    df['NEW_CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    ## delete unused dataand clean up memory\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv('./input/bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv('./input/bureau_balance.csv', nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    cols = active_agg.columns.tolist()\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    for e in cols:\n",
    "        bureau_agg['NEW_RATIO_BURO_' + e[0] + \"_\" + e[1].upper()] = bureau_agg['ACTIVE_' + e[0] + \"_\" + e[1].upper()] / bureau_agg['CLOSED_' + e[0] + \"_\" + e[1].upper()]\n",
    "    \n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg\n",
    "\n",
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    prev = pd.read_csv('./input/previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    cols = approved_agg.columns.tolist()\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    \n",
    "    for e in cols:\n",
    "        prev_agg['NEW_RATIO_PREV_' + e[0] + \"_\" + e[1].upper()] = prev_agg['APPROVED_' + e[0] + \"_\" + e[1].upper()] / prev_agg['REFUSED_' + e[0] + \"_\" + e[1].upper()]\n",
    "    \n",
    "    gc.collect()\n",
    "    return prev_agg\n",
    "\n",
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv('./input/POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg\n",
    "    \n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv('./input/installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    cc = pd.read_csv('./input/credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display/plot feature importance by average results from k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/plot feature importance from k-fold cross-validation\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout\n",
    "    plt.savefig('lgbm_importances01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unused features: 339\n"
     ]
    }
   ],
   "source": [
    "## Remove unimportant features by feature selection\n",
    "features_with_no_imp_at_least_twice = [\n",
    "    'ACTIVE_CNT_CREDIT_PROLONG_SUM', 'ACTIVE_CREDIT_DAY_OVERDUE_MEAN', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "    'AMT_REQ_CREDIT_BUREAU_WEEK', 'BURO_CNT_CREDIT_PROLONG_SUM', 'BURO_CREDIT_ACTIVE_Bad debt_MEAN', 'BURO_CREDIT_ACTIVE_nan_MEAN',\n",
    "    'BURO_CREDIT_CURRENCY_currency 1_MEAN', 'BURO_CREDIT_CURRENCY_currency 2_MEAN', 'BURO_CREDIT_CURRENCY_currency 3_MEAN',\n",
    "    'BURO_CREDIT_CURRENCY_currency 4_MEAN', 'BURO_CREDIT_CURRENCY_nan_MEAN', 'BURO_CREDIT_DAY_OVERDUE_MAX', 'BURO_CREDIT_DAY_OVERDUE_MEAN',\n",
    "    'BURO_CREDIT_TYPE_Cash loan (non-earmarked)_MEAN', 'BURO_CREDIT_TYPE_Interbank credit_MEAN', 'BURO_CREDIT_TYPE_Loan for business development_MEAN',\n",
    "    'BURO_CREDIT_TYPE_Loan for purchase of shares (margin lending)_MEAN', 'BURO_CREDIT_TYPE_Loan for the purchase of equipment_MEAN',\n",
    "    'BURO_CREDIT_TYPE_Loan for working capital replenishment_MEAN', 'BURO_CREDIT_TYPE_Mobile operator loan_MEAN',\n",
    "    'BURO_CREDIT_TYPE_Real estate loan_MEAN', 'BURO_CREDIT_TYPE_Unknown type of loan_MEAN', 'BURO_CREDIT_TYPE_nan_MEAN',\n",
    "    'BURO_MONTHS_BALANCE_MAX_MAX', 'BURO_STATUS_2_MEAN_MEAN', 'BURO_STATUS_3_MEAN_MEAN', 'BURO_STATUS_4_MEAN_MEAN', 'BURO_STATUS_5_MEAN_MEAN',\n",
    "    'BURO_STATUS_nan_MEAN_MEAN', 'CC_AMT_DRAWINGS_ATM_CURRENT_MIN', 'CC_AMT_DRAWINGS_CURRENT_MIN', 'CC_AMT_DRAWINGS_OTHER_CURRENT_MAX',\n",
    "    'CC_AMT_DRAWINGS_OTHER_CURRENT_MEAN', 'CC_AMT_DRAWINGS_OTHER_CURRENT_MIN', 'CC_AMT_DRAWINGS_OTHER_CURRENT_SUM',\n",
    "    'CC_AMT_DRAWINGS_OTHER_CURRENT_VAR', 'CC_AMT_INST_MIN_REGULARITY_MIN', 'CC_AMT_PAYMENT_TOTAL_CURRENT_MIN', 'CC_AMT_PAYMENT_TOTAL_CURRENT_VAR',\n",
    "    'CC_AMT_RECIVABLE_SUM', 'CC_AMT_TOTAL_RECEIVABLE_MAX', 'CC_AMT_TOTAL_RECEIVABLE_MIN', 'CC_AMT_TOTAL_RECEIVABLE_SUM', 'CC_AMT_TOTAL_RECEIVABLE_VAR',\n",
    "    'CC_CNT_DRAWINGS_ATM_CURRENT_MIN', 'CC_CNT_DRAWINGS_CURRENT_MIN', 'CC_CNT_DRAWINGS_OTHER_CURRENT_MAX', 'CC_CNT_DRAWINGS_OTHER_CURRENT_MEAN',\n",
    "    'CC_CNT_DRAWINGS_OTHER_CURRENT_MIN', 'CC_CNT_DRAWINGS_OTHER_CURRENT_SUM', 'CC_CNT_DRAWINGS_OTHER_CURRENT_VAR', 'CC_CNT_DRAWINGS_POS_CURRENT_SUM',\n",
    "    'CC_CNT_INSTALMENT_MATURE_CUM_MAX', 'CC_CNT_INSTALMENT_MATURE_CUM_MIN', 'CC_COUNT', 'CC_MONTHS_BALANCE_MAX', 'CC_MONTHS_BALANCE_MEAN',\n",
    "    'CC_MONTHS_BALANCE_MIN', 'CC_MONTHS_BALANCE_SUM', 'CC_NAME_CONTRACT_STATUS_Active_MAX', 'CC_NAME_CONTRACT_STATUS_Active_MIN',\n",
    "    'CC_NAME_CONTRACT_STATUS_Approved_MAX', 'CC_NAME_CONTRACT_STATUS_Approved_MEAN', 'CC_NAME_CONTRACT_STATUS_Approved_MIN',\n",
    "    'CC_NAME_CONTRACT_STATUS_Approved_SUM', 'CC_NAME_CONTRACT_STATUS_Approved_VAR', 'CC_NAME_CONTRACT_STATUS_Completed_MAX',\n",
    "    'CC_NAME_CONTRACT_STATUS_Completed_MEAN', 'CC_NAME_CONTRACT_STATUS_Completed_MIN', 'CC_NAME_CONTRACT_STATUS_Completed_SUM', 'CC_NAME_CONTRACT_STATUS_Completed_VAR',\n",
    "    'CC_NAME_CONTRACT_STATUS_Demand_MAX', 'CC_NAME_CONTRACT_STATUS_Demand_MEAN', 'CC_NAME_CONTRACT_STATUS_Demand_MIN', 'CC_NAME_CONTRACT_STATUS_Demand_SUM',\n",
    "    'CC_NAME_CONTRACT_STATUS_Demand_VAR', 'CC_NAME_CONTRACT_STATUS_Refused_MAX', 'CC_NAME_CONTRACT_STATUS_Refused_MEAN', 'CC_NAME_CONTRACT_STATUS_Refused_MIN',\n",
    "    'CC_NAME_CONTRACT_STATUS_Refused_SUM', 'CC_NAME_CONTRACT_STATUS_Refused_VAR', 'CC_NAME_CONTRACT_STATUS_Sent proposal_MAX',\n",
    "    'CC_NAME_CONTRACT_STATUS_Sent proposal_MEAN', 'CC_NAME_CONTRACT_STATUS_Sent proposal_MIN', 'CC_NAME_CONTRACT_STATUS_Sent proposal_SUM',\n",
    "    'CC_NAME_CONTRACT_STATUS_Sent proposal_VAR', 'CC_NAME_CONTRACT_STATUS_Signed_MAX', 'CC_NAME_CONTRACT_STATUS_Signed_MEAN', 'CC_NAME_CONTRACT_STATUS_Signed_MIN',\n",
    "    'CC_NAME_CONTRACT_STATUS_Signed_SUM', 'CC_NAME_CONTRACT_STATUS_Signed_VAR', 'CC_NAME_CONTRACT_STATUS_nan_MAX', 'CC_NAME_CONTRACT_STATUS_nan_MEAN',\n",
    "    'CC_NAME_CONTRACT_STATUS_nan_MIN', 'CC_NAME_CONTRACT_STATUS_nan_SUM', 'CC_NAME_CONTRACT_STATUS_nan_VAR', 'CC_SK_DPD_DEF_MAX',\n",
    "    'CC_SK_DPD_DEF_MIN', 'CC_SK_DPD_DEF_SUM', 'CC_SK_DPD_DEF_VAR', 'CC_SK_DPD_MAX', 'CC_SK_DPD_MEAN', 'CC_SK_DPD_MIN', 'CC_SK_DPD_SUM',\n",
    "    'CC_SK_DPD_VAR', 'CLOSED_AMT_CREDIT_SUM_LIMIT_MEAN', 'CLOSED_AMT_CREDIT_SUM_LIMIT_SUM', 'CLOSED_AMT_CREDIT_SUM_OVERDUE_MEAN',\n",
    "    'CLOSED_CNT_CREDIT_PROLONG_SUM', 'CLOSED_CREDIT_DAY_OVERDUE_MAX', 'CLOSED_CREDIT_DAY_OVERDUE_MEAN', 'CLOSED_MONTHS_BALANCE_MAX_MAX',\n",
    "    'CNT_CHILDREN', 'ELEVATORS_MEDI', 'ELEVATORS_MODE', 'EMERGENCYSTATE_MODE_No', 'EMERGENCYSTATE_MODE_Yes', 'ENTRANCES_MODE', 'FLAG_CONT_MOBILE',\n",
    "    'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16',\n",
    "    'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n",
    "    'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_9', 'FLAG_EMAIL', 'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_OWN_CAR', 'FLOORSMAX_MODE',\n",
    "    'FONDKAPREMONT_MODE_not specified', 'FONDKAPREMONT_MODE_org spec account', 'FONDKAPREMONT_MODE_reg oper account', 'FONDKAPREMONT_MODE_reg oper spec account',\n",
    "    'HOUSETYPE_MODE_block of flats', 'HOUSETYPE_MODE_specific housing', 'HOUSETYPE_MODE_terraced house', 'LIVE_REGION_NOT_WORK_REGION',\n",
    "    'NAME_CONTRACT_TYPE_Revolving loans', 'NAME_EDUCATION_TYPE_Academic degree', 'NAME_FAMILY_STATUS_Civil marriage', 'NAME_FAMILY_STATUS_Single / not married',\n",
    "    'NAME_FAMILY_STATUS_Unknown', 'NAME_FAMILY_STATUS_Widow', 'NAME_HOUSING_TYPE_Co-op apartment', 'NAME_HOUSING_TYPE_With parents',\n",
    "    'NAME_INCOME_TYPE_Businessman', 'NAME_INCOME_TYPE_Maternity leave', 'NAME_INCOME_TYPE_Pensioner', 'NAME_INCOME_TYPE_Student',\n",
    "    'NAME_INCOME_TYPE_Unemployed', 'NAME_TYPE_SUITE_Children', 'NAME_TYPE_SUITE_Family', 'NAME_TYPE_SUITE_Group of people',\n",
    "    'NAME_TYPE_SUITE_Other_A', 'NAME_TYPE_SUITE_Other_B', 'NAME_TYPE_SUITE_Spouse, partner', 'NAME_TYPE_SUITE_Unaccompanied',\n",
    "    'NEW_RATIO_BURO_AMT_CREDIT_SUM_DEBT_MEAN', 'NEW_RATIO_BURO_AMT_CREDIT_SUM_LIMIT_SUM', 'NEW_RATIO_BURO_AMT_CREDIT_SUM_OVERDUE_MEAN',\n",
    "    'NEW_RATIO_BURO_CNT_CREDIT_PROLONG_SUM', 'NEW_RATIO_BURO_CREDIT_DAY_OVERDUE_MAX', 'NEW_RATIO_BURO_CREDIT_DAY_OVERDUE_MEAN', 'NEW_RATIO_BURO_MONTHS_BALANCE_MAX_MAX',\n",
    "    'NEW_RATIO_PREV_AMT_DOWN_PAYMENT_MIN', 'NEW_RATIO_PREV_RATE_DOWN_PAYMENT_MAX', 'OCCUPATION_TYPE_Cleaning staff', 'OCCUPATION_TYPE_Cooking staff',\n",
    "    'OCCUPATION_TYPE_HR staff', 'OCCUPATION_TYPE_IT staff', 'OCCUPATION_TYPE_Low-skill Laborers', 'OCCUPATION_TYPE_Managers',\n",
    "    'OCCUPATION_TYPE_Private service staff', 'OCCUPATION_TYPE_Realty agents', 'OCCUPATION_TYPE_Sales staff', 'OCCUPATION_TYPE_Secretaries',\n",
    "    'OCCUPATION_TYPE_Security staff', 'OCCUPATION_TYPE_Waiters/barmen staff', 'ORGANIZATION_TYPE_Advertising', 'ORGANIZATION_TYPE_Agriculture',\n",
    "    'ORGANIZATION_TYPE_Business Entity Type 1', 'ORGANIZATION_TYPE_Business Entity Type 2', 'ORGANIZATION_TYPE_Cleaning', 'ORGANIZATION_TYPE_Culture',\n",
    "    'ORGANIZATION_TYPE_Electricity', 'ORGANIZATION_TYPE_Emergency', 'ORGANIZATION_TYPE_Government', 'ORGANIZATION_TYPE_Hotel', 'ORGANIZATION_TYPE_Housing',\n",
    "    'ORGANIZATION_TYPE_Industry: type 1', 'ORGANIZATION_TYPE_Industry: type 10', 'ORGANIZATION_TYPE_Industry: type 11', 'ORGANIZATION_TYPE_Industry: type 12',\n",
    "    'ORGANIZATION_TYPE_Industry: type 13', 'ORGANIZATION_TYPE_Industry: type 2', 'ORGANIZATION_TYPE_Industry: type 3', 'ORGANIZATION_TYPE_Industry: type 4',\n",
    "    'ORGANIZATION_TYPE_Industry: type 5', 'ORGANIZATION_TYPE_Industry: type 6', 'ORGANIZATION_TYPE_Industry: type 7', 'ORGANIZATION_TYPE_Industry: type 8',\n",
    "    'ORGANIZATION_TYPE_Insurance', 'ORGANIZATION_TYPE_Legal Services', 'ORGANIZATION_TYPE_Mobile', 'ORGANIZATION_TYPE_Other', 'ORGANIZATION_TYPE_Postal',\n",
    "    'ORGANIZATION_TYPE_Realtor', 'ORGANIZATION_TYPE_Religion', 'ORGANIZATION_TYPE_Restaurant', 'ORGANIZATION_TYPE_Security',\n",
    "    'ORGANIZATION_TYPE_Security Ministries', 'ORGANIZATION_TYPE_Services', 'ORGANIZATION_TYPE_Telecom', 'ORGANIZATION_TYPE_Trade: type 1',\n",
    "    'ORGANIZATION_TYPE_Trade: type 2', 'ORGANIZATION_TYPE_Trade: type 3', 'ORGANIZATION_TYPE_Trade: type 4', 'ORGANIZATION_TYPE_Trade: type 5',\n",
    "    'ORGANIZATION_TYPE_Trade: type 6', 'ORGANIZATION_TYPE_Trade: type 7',\n",
    "    'ORGANIZATION_TYPE_Transport: type 1', 'ORGANIZATION_TYPE_Transport: type 2', 'ORGANIZATION_TYPE_Transport: type 4', 'ORGANIZATION_TYPE_University',\n",
    "    'ORGANIZATION_TYPE_XNA', 'POS_NAME_CONTRACT_STATUS_Amortized debt_MEAN', 'POS_NAME_CONTRACT_STATUS_Approved_MEAN', 'POS_NAME_CONTRACT_STATUS_Canceled_MEAN',\n",
    "    'POS_NAME_CONTRACT_STATUS_Demand_MEAN', 'POS_NAME_CONTRACT_STATUS_XNA_MEAN', 'POS_NAME_CONTRACT_STATUS_nan_MEAN', 'PREV_CHANNEL_TYPE_Car dealer_MEAN',\n",
    "    'PREV_CHANNEL_TYPE_nan_MEAN', 'PREV_CODE_REJECT_REASON_CLIENT_MEAN', 'PREV_CODE_REJECT_REASON_SYSTEM_MEAN', 'PREV_CODE_REJECT_REASON_VERIF_MEAN',\n",
    "    'PREV_CODE_REJECT_REASON_XNA_MEAN', 'PREV_CODE_REJECT_REASON_nan_MEAN', 'PREV_FLAG_LAST_APPL_PER_CONTRACT_N_MEAN', 'PREV_FLAG_LAST_APPL_PER_CONTRACT_Y_MEAN',\n",
    "    'PREV_FLAG_LAST_APPL_PER_CONTRACT_nan_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Building a house or an annex_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Business development_MEAN',\n",
    "    'PREV_NAME_CASH_LOAN_PURPOSE_Buying a garage_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Buying a holiday home / land_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Buying a home_MEAN',\n",
    "    'PREV_NAME_CASH_LOAN_PURPOSE_Buying a new car_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Buying a used car_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Education_MEAN',\n",
    "    'PREV_NAME_CASH_LOAN_PURPOSE_Everyday expenses_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Furniture_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Gasification / water supply_MEAN',\n",
    "    'PREV_NAME_CASH_LOAN_PURPOSE_Hobby_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Journey_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Money for a third person_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Other_MEAN',\n",
    "    'PREV_NAME_CASH_LOAN_PURPOSE_Payments on other loans_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Purchase of electronic equipment_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_Refusal to name the goal_MEAN',\n",
    "    'PREV_NAME_CASH_LOAN_PURPOSE_Wedding / gift / holiday_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_XAP_MEAN', 'PREV_NAME_CASH_LOAN_PURPOSE_nan_MEAN', 'PREV_NAME_CLIENT_TYPE_XNA_MEAN',\n",
    "    'PREV_NAME_CLIENT_TYPE_nan_MEAN', 'PREV_NAME_CONTRACT_STATUS_Unused offer_MEAN', 'PREV_NAME_CONTRACT_STATUS_nan_MEAN', 'PREV_NAME_CONTRACT_TYPE_XNA_MEAN',\n",
    "    'PREV_NAME_CONTRACT_TYPE_nan_MEAN', 'PREV_NAME_GOODS_CATEGORY_Additional Service_MEAN', 'PREV_NAME_GOODS_CATEGORY_Animals_MEAN',\n",
    "    'PREV_NAME_GOODS_CATEGORY_Auto Accessories_MEAN', 'PREV_NAME_GOODS_CATEGORY_Clothing and Accessories_MEAN', 'PREV_NAME_GOODS_CATEGORY_Construction Materials_MEAN',\n",
    "    'PREV_NAME_GOODS_CATEGORY_Direct Sales_MEAN', 'PREV_NAME_GOODS_CATEGORY_Education_MEAN', 'PREV_NAME_GOODS_CATEGORY_Fitness_MEAN',\n",
    "    'PREV_NAME_GOODS_CATEGORY_Gardening_MEAN', 'PREV_NAME_GOODS_CATEGORY_Homewares_MEAN', 'PREV_NAME_GOODS_CATEGORY_House Construction_MEAN',\n",
    "    'PREV_NAME_GOODS_CATEGORY_Insurance_MEAN', 'PREV_NAME_GOODS_CATEGORY_Jewelry_MEAN', 'PREV_NAME_GOODS_CATEGORY_Medical Supplies_MEAN',\n",
    "    'PREV_NAME_GOODS_CATEGORY_Medicine_MEAN', 'PREV_NAME_GOODS_CATEGORY_Office Appliances_MEAN', 'PREV_NAME_GOODS_CATEGORY_Other_MEAN', 'PREV_NAME_GOODS_CATEGORY_Tourism_MEAN',\n",
    "    'PREV_NAME_GOODS_CATEGORY_Vehicles_MEAN', 'PREV_NAME_GOODS_CATEGORY_Weapon_MEAN', 'PREV_NAME_GOODS_CATEGORY_XNA_MEAN', 'PREV_NAME_GOODS_CATEGORY_nan_MEAN',\n",
    "    'PREV_NAME_PAYMENT_TYPE_Cashless from the account of the employer_MEAN', 'PREV_NAME_PAYMENT_TYPE_Non-cash from your account_MEAN', 'PREV_NAME_PAYMENT_TYPE_nan_MEAN',\n",
    "    'PREV_NAME_PORTFOLIO_Cars_MEAN', 'PREV_NAME_PORTFOLIO_nan_MEAN', 'PREV_NAME_PRODUCT_TYPE_nan_MEAN', 'PREV_NAME_SELLER_INDUSTRY_Construction_MEAN',\n",
    "    'PREV_NAME_SELLER_INDUSTRY_Furniture_MEAN', 'PREV_NAME_SELLER_INDUSTRY_Industry_MEAN', 'PREV_NAME_SELLER_INDUSTRY_Jewelry_MEAN', 'PREV_NAME_SELLER_INDUSTRY_MLM partners_MEAN',\n",
    "    'PREV_NAME_SELLER_INDUSTRY_Tourism_MEAN', 'PREV_NAME_SELLER_INDUSTRY_nan_MEAN', 'PREV_NAME_TYPE_SUITE_Group of people_MEAN', 'PREV_NAME_YIELD_GROUP_nan_MEAN',\n",
    "    'PREV_PRODUCT_COMBINATION_POS industry without interest_MEAN', 'PREV_PRODUCT_COMBINATION_POS mobile without interest_MEAN', 'PREV_PRODUCT_COMBINATION_POS others without interest_MEAN',\n",
    "    'PREV_PRODUCT_COMBINATION_nan_MEAN', 'PREV_WEEKDAY_APPR_PROCESS_START_nan_MEAN', 'REFUSED_AMT_DOWN_PAYMENT_MAX', 'REFUSED_AMT_DOWN_PAYMENT_MEAN',\n",
    "    'REFUSED_RATE_DOWN_PAYMENT_MIN', 'REG_CITY_NOT_WORK_CITY', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n",
    "    'WALLSMATERIAL_MODE_Block', 'WALLSMATERIAL_MODE_Mixed', 'WALLSMATERIAL_MODE_Monolithic', 'WALLSMATERIAL_MODE_Others', 'WALLSMATERIAL_MODE_Panel',\n",
    "    'WALLSMATERIAL_MODE_Wooden', 'WEEKDAY_APPR_PROCESS_START_FRIDAY', 'WEEKDAY_APPR_PROCESS_START_THURSDAY', 'WEEKDAY_APPR_PROCESS_START_TUESDAY'\n",
    "]\n",
    "print(f\"Number of unused features: {len(features_with_no_imp_at_least_twice)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge/join different tables on SK_ID_CURR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(debug = False):\n",
    "    num_rows = 20000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    with timer(\"prepare train and test data\"):\n",
    "        print(df.shape)\n",
    "        drop_columns = [col for col in features_with_no_imp_at_least_twice if col in df.columns]\n",
    "        print(len(drop_columns),len(df.columns))\n",
    "        #df.drop(features_with_no_imp_at_least_twice, axis=1, inplace=True)\n",
    "        df.drop(drop_columns, axis=1, inplace=True)\n",
    "        train_df = df[df['TARGET'].notnull()]\n",
    "        test_df = df[df['TARGET'].isnull()]\n",
    "        del df\n",
    "        gc.collect()\n",
    "        #print(df.shape)\n",
    "        \n",
    "    return     (train_df, test_df) ## train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and test data\n",
    "- we can set debug=True to sample a subset for faster debugging, feature engineering/selection, tuning parameters purpose etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 20000, test samples: 20000\n",
      "Bureau df shape: (3990, 136)\n",
      "Process bureau and bureau_balance - done in 1s\n",
      "Previous applications df shape: (19006, 275)\n",
      "Process previous_applications - done in 1s\n",
      "Pos-cash balance df shape: (18534, 15)\n",
      "Process POS-CASH balance - done in 0s\n",
      "Installments payments df shape: (16356, 26)\n",
      "Process installments payments - done in 0s\n",
      "Credit card balance df shape: (17659, 131)\n",
      "Process credit card balance - done in 1s\n",
      "(40000, 844)\n",
      "313 844\n",
      "prepare train and test data - done in 0s\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = prepare_data(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four steps for Bayesian optimization for hyper-parameter tunning\n",
    "- Step 1. Specify parameters with bounds to be optimized\n",
    "- Step 2. Define an object function with the optimized parameters from Step 1\n",
    "- Step 3. Create an Bayesian optimization object by specifying optimized parameters from Step 1 and the object function from Step 2\n",
    "- Step 4. Run the Bayesian optimization object from Step 3 by specifying init_number and  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: parameters to be tuned/optimized by Bayesian optimization\n",
    "- NOTE: the parameters must be matched and passed into the evaluation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter bounds for tuning\n",
    "pbounds = {\n",
    "    'n_estimators': (800,2000),\n",
    "    'max_depth': (6,10),\n",
    "    'subsample': (0.7, 0.9),  # Change for big datasets\n",
    "    'colsample_bytree': (0.7, 0.98), # Change for datasets with lots of features\n",
    "    'gamma':(0.001, 10.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data set for xgboost, which will be used in the object/evaluation function\n",
    "- X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in train_df.columns]\n",
    "feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']] \n",
    "X_train = train_df[feats]\n",
    "y_train = train_df['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Define an object function with the optimized parameters from Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation/optimization function with tuned parameters: \n",
    "def xgboost_CV_eval_fun (n_estimators,\n",
    "                        max_depth,\n",
    "                        subsample,\n",
    "                        colsample_bytree,\n",
    "                        gamma):\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators= int(n_estimators), ## integer , parameter   \n",
    "        max_depth = int(max_depth), ## parameter\n",
    "        learning_rate = 0.02, ## fixed\n",
    "        gamma = max(gamma,0), ## parameter, penalized hessian\n",
    "        colsample_bytree = min(colsample_bytree,1), ## fraction\n",
    "        eval_metric = 'auc',\n",
    "        # USE CPU\n",
    "        #nthread=4,\n",
    "        #tree_method='hist' \n",
    "        # USE GPU\n",
    "        tree_method='gpu_hist' ## run on gpu         \n",
    "    )\n",
    "    # Bayesian optimization only knows how to maximize, not minimize.\n",
    "    # For MSE, we need return negative MSE; For auc, we return its original value   \n",
    "    return np.mean(cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Create an Bayesian optimization object\n",
    "- Initiate a BayesianOptimization object by specifying an optimized 'f', and its parameters'pbounds' with given bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo = BayesianOptimization(f=xgboost_CV_eval_fun, pbounds=pbounds, random_state=51620)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Run the Bayesian optimization object With specifying initial number for exploration and the iteration number\n",
    "- n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "- init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n",
    "- the total interation is init_points + n_iter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7486  \u001b[0m | \u001b[0m 0.8318  \u001b[0m | \u001b[0m 3.059   \u001b[0m | \u001b[0m 6.164   \u001b[0m | \u001b[0m 1.881e+0\u001b[0m | \u001b[0m 0.8078  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7431  \u001b[0m | \u001b[0m 0.9492  \u001b[0m | \u001b[0m 0.8401  \u001b[0m | \u001b[0m 9.825   \u001b[0m | \u001b[0m 1.624e+0\u001b[0m | \u001b[0m 0.8875  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7455  \u001b[0m | \u001b[0m 0.7792  \u001b[0m | \u001b[0m 1.442   \u001b[0m | \u001b[0m 8.4     \u001b[0m | \u001b[0m 1.475e+0\u001b[0m | \u001b[0m 0.787   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7471  \u001b[0m | \u001b[0m 0.8063  \u001b[0m | \u001b[0m 3.178   \u001b[0m | \u001b[0m 6.961   \u001b[0m | \u001b[0m 1.053e+0\u001b[0m | \u001b[0m 0.835   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7438  \u001b[0m | \u001b[0m 0.706   \u001b[0m | \u001b[0m 9.981   \u001b[0m | \u001b[0m 7.79    \u001b[0m | \u001b[0m 1.197e+0\u001b[0m | \u001b[0m 0.8491  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7425  \u001b[0m | \u001b[0m 0.9496  \u001b[0m | \u001b[0m 0.996   \u001b[0m | \u001b[0m 6.02    \u001b[0m | \u001b[0m 803.6   \u001b[0m | \u001b[0m 0.8631  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.735   \u001b[0m | \u001b[0m 0.8423  \u001b[0m | \u001b[0m 0.3525  \u001b[0m | \u001b[0m 6.23    \u001b[0m | \u001b[0m 1.997e+0\u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7405  \u001b[0m | \u001b[0m 0.7951  \u001b[0m | \u001b[0m 0.6687  \u001b[0m | \u001b[0m 6.048   \u001b[0m | \u001b[0m 1.801e+0\u001b[0m | \u001b[0m 0.8274  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7438  \u001b[0m | \u001b[0m 0.9596  \u001b[0m | \u001b[0m 9.602   \u001b[0m | \u001b[0m 9.929   \u001b[0m | \u001b[0m 1.894e+0\u001b[0m | \u001b[0m 0.7948  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.744   \u001b[0m | \u001b[0m 0.7818  \u001b[0m | \u001b[0m 9.664   \u001b[0m | \u001b[0m 6.095   \u001b[0m | \u001b[0m 920.7   \u001b[0m | \u001b[0m 0.7882  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7367  \u001b[0m | \u001b[0m 0.9737  \u001b[0m | \u001b[0m 0.005012\u001b[0m | \u001b[0m 6.238   \u001b[0m | \u001b[0m 1.306e+0\u001b[0m | \u001b[0m 0.8291  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7443  \u001b[0m | \u001b[0m 0.9581  \u001b[0m | \u001b[0m 9.823   \u001b[0m | \u001b[0m 6.019   \u001b[0m | \u001b[0m 1.57e+03\u001b[0m | \u001b[0m 0.8427  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7444  \u001b[0m | \u001b[0m 0.879   \u001b[0m | \u001b[0m 9.439   \u001b[0m | \u001b[0m 9.857   \u001b[0m | \u001b[0m 1.056e+0\u001b[0m | \u001b[0m 0.7788  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7331  \u001b[0m | \u001b[0m 0.8186  \u001b[0m | \u001b[0m 0.04943 \u001b[0m | \u001b[0m 6.439   \u001b[0m | \u001b[0m 1.896e+0\u001b[0m | \u001b[0m 0.7329  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7437  \u001b[0m | \u001b[0m 0.7     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 6.0     \u001b[0m | \u001b[0m 1.853e+0\u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.7323  \u001b[0m | \u001b[0m 0.9628  \u001b[0m | \u001b[0m 0.1329  \u001b[0m | \u001b[0m 9.404   \u001b[0m | \u001b[0m 1.545e+0\u001b[0m | \u001b[0m 0.7913  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.7438  \u001b[0m | \u001b[0m 0.9546  \u001b[0m | \u001b[0m 9.475   \u001b[0m | \u001b[0m 6.413   \u001b[0m | \u001b[0m 1.396e+0\u001b[0m | \u001b[0m 0.8717  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7315  \u001b[0m | \u001b[0m 0.9692  \u001b[0m | \u001b[0m 0.163   \u001b[0m | \u001b[0m 6.406   \u001b[0m | \u001b[0m 1.699e+0\u001b[0m | \u001b[0m 0.8277  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.7464  \u001b[0m | \u001b[0m 0.9608  \u001b[0m | \u001b[0m 5.16    \u001b[0m | \u001b[0m 7.848   \u001b[0m | \u001b[0m 1.884e+0\u001b[0m | \u001b[0m 0.8036  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.744   \u001b[0m | \u001b[0m 0.7938  \u001b[0m | \u001b[0m 9.952   \u001b[0m | \u001b[0m 9.523   \u001b[0m | \u001b[0m 1.999e+0\u001b[0m | \u001b[0m 0.7503  \u001b[0m |\n",
      "=====================================================================================\n",
      "Bayesian Optimization - done in 561s\n"
     ]
    }
   ],
   "source": [
    "## Optimize model parameters by Bayesian optimizer to improve model performance\n",
    "with timer(\"Bayesian Optimization\"):\n",
    "    xgb_bo.maximize(init_points=5, n_iter=15, acq='ei')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the parameters of the best model\n",
    "- xgb_bo.res : see all results\n",
    "- xgb_max: extract the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model from tuning: {'target': 0.7485832162748585, 'params': {'colsample_bytree': 0.8318456460242112, 'gamma': 3.0594505844428506, 'max_depth': 6.163576436526432, 'n_estimators': 1881.2395736545425, 'subsample': 0.8078142393826673}}\n"
     ]
    }
   ],
   "source": [
    "xgb_params_best = xgb_bo.max['params'] \n",
    "print(f\"The best model from tuning:\",xgb_bo.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep improving models by adjust new bounds to fine-tune parameters\n",
    "- here, by adjusting max_depth, we can achieve better model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current max_depth, 6.163576436526432\n",
      "new bound on max_depth:(5.163576436526432, 7.163576436526432)\n",
      "new bound on subsample:(0.7078142393826673, 0.9078142393826673)\n",
      "new bound on colsample_bytree:(0.7318456460242112, 0.9318456460242112)\n"
     ]
    }
   ],
   "source": [
    "##  Focus on smaller bounds and do fine-tuning: We can update the bounds and focus more on fine-tunning.\n",
    "print(f\"current max_depth, {xgb_params_best['max_depth']}\")\n",
    "newBound_max_depth= (xgb_params_best['max_depth']-1, xgb_params_best['max_depth']+1)\n",
    "newBound_subsample = (xgb_params_best['subsample']-0.1, xgb_params_best['subsample']+0.1)\n",
    "newBound_colsample_bytree = (xgb_params_best['colsample_bytree']-0.1, xgb_params_best['colsample_bytree']+0.1)\n",
    "print(f\"new bound on max_depth:{newBound_max_depth}\")\n",
    "print(f\"new bound on subsample:{newBound_subsample}\")\n",
    "print(f\"new bound on colsample_bytree:{newBound_colsample_bytree}\")\n",
    "xgb_bo.set_bounds(new_bounds={\"max_depth\": newBound_max_depth, \n",
    "                              \"subsample\":newBound_subsample, \n",
    "                              \"colsample_bytree\":newBound_colsample_bytree}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.7427  \u001b[0m | \u001b[0m 0.9152  \u001b[0m | \u001b[0m 0.3005  \u001b[0m | \u001b[0m 5.361   \u001b[0m | \u001b[0m 1.125e+0\u001b[0m | \u001b[0m 0.797   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.7376  \u001b[0m | \u001b[0m 0.8581  \u001b[0m | \u001b[0m 0.01206 \u001b[0m | \u001b[0m 7.012   \u001b[0m | \u001b[0m 981.9   \u001b[0m | \u001b[0m 0.7552  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.7435  \u001b[0m | \u001b[0m 0.9127  \u001b[0m | \u001b[0m 0.03851 \u001b[0m | \u001b[0m 5.381   \u001b[0m | \u001b[0m 861.1   \u001b[0m | \u001b[0m 0.7486  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.7438  \u001b[0m | \u001b[0m 0.8015  \u001b[0m | \u001b[0m 9.693   \u001b[0m | \u001b[0m 5.509   \u001b[0m | \u001b[0m 1.461e+0\u001b[0m | \u001b[0m 0.7598  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.7401  \u001b[0m | \u001b[0m 0.7355  \u001b[0m | \u001b[0m 0.155   \u001b[0m | \u001b[0m 5.263   \u001b[0m | \u001b[0m 1.418e+0\u001b[0m | \u001b[0m 0.7297  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.7438  \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 9.979   \u001b[0m | \u001b[0m 6.168   \u001b[0m | \u001b[0m 832.4   \u001b[0m | \u001b[0m 0.8961  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.7437  \u001b[0m | \u001b[0m 0.8295  \u001b[0m | \u001b[0m 9.827   \u001b[0m | \u001b[0m 5.333   \u001b[0m | \u001b[0m 1.075e+0\u001b[0m | \u001b[0m 0.8843  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.7372  \u001b[0m | \u001b[0m 0.8732  \u001b[0m | \u001b[0m 0.1238  \u001b[0m | \u001b[0m 7.14    \u001b[0m | \u001b[0m 1.226e+0\u001b[0m | \u001b[0m 0.8077  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7446  \u001b[0m | \u001b[0m 0.8665  \u001b[0m | \u001b[0m 0.9904  \u001b[0m | \u001b[0m 6.967   \u001b[0m | \u001b[0m 1.866e+0\u001b[0m | \u001b[0m 0.8664  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.7438  \u001b[0m | \u001b[0m 0.9132  \u001b[0m | \u001b[0m 9.4     \u001b[0m | \u001b[0m 5.69    \u001b[0m | \u001b[0m 1.88e+03\u001b[0m | \u001b[0m 0.8421  \u001b[0m |\n",
      "=====================================================================================\n",
      "Fine-tuning by Bayesian Optimization - done in 235s\n"
     ]
    }
   ],
   "source": [
    "## We then can tune the new bounds with existing parameters. \n",
    "## No init iteration is needed anymore\n",
    "with timer(\"Fine-tuning by Bayesian Optimization\"):\n",
    "    xgb_bo.maximize(init_points=0, n_iter=10, acq='ei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model after fine-tuning: {'target': 0.7485832162748585, 'params': {'colsample_bytree': 0.8318456460242112, 'gamma': 3.0594505844428506, 'max_depth': 6.163576436526432, 'n_estimators': 1881.2395736545425, 'subsample': 0.8078142393826673}}\n"
     ]
    }
   ],
   "source": [
    "xgb_params_best = xgb_bo.max['params'] \n",
    "print(f\"The best model after fine-tuning:\",xgb_bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After we finish hyper-parameter search, we can use these best model parameters to run k-fold on the all training data and obtain prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "Bureau df shape: (305811, 143)\n",
      "Process bureau and bureau_balance - done in 23s\n",
      "Previous applications df shape: (338857, 279)\n",
      "Process previous_applications - done in 27s\n",
      "Pos-cash balance df shape: (337252, 18)\n",
      "Process POS-CASH balance - done in 10s\n",
      "Installments payments df shape: (339587, 26)\n",
      "Process installments payments - done in 24s\n",
      "Credit card balance df shape: (103558, 141)\n",
      "Process credit card balance - done in 15s\n",
      "(356251, 870)\n",
      "339 870\n",
      "prepare train and test data - done in 3s\n",
      "Fine-tuning by Bayesian Optimization - done in 109s\n"
     ]
    }
   ],
   "source": [
    "## obtain the whole training and test data\n",
    "with timer(\"Fine-tuning by Bayesian Optimization\"):\n",
    "    all_train_df, all_test_df = prepare_data(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train k-fold validation on the best parameters from hyperparameter tuning with Bayesian optimization\n",
    "def kfold_xgboost(train_df, test_df, num_folds, params=None,stratified = False, debug= False):    \n",
    "    clf = xgb.XGBClassifier( \n",
    "        n_estimators=2000,\n",
    "        max_depth=10, \n",
    "        learning_rate=0.02, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8, \n",
    "        #missing=-1, \n",
    "        eval_metric='auc',\n",
    "        # USE CPU\n",
    "        #nthread=4,\n",
    "        #tree_method='hist' \n",
    "        # USE GPU\n",
    "        tree_method='gpu_hist' \n",
    "    )\n",
    "    if params:\n",
    "        print(f\"Parameters from Bayesian optimization tuning: {params}\")\n",
    "        clf = xgb.XGBClassifier( \n",
    "            n_estimators=max(2000,int(params[\"n_estimators\"])),\n",
    "            max_depth=int(params[\"max_depth\"]), \n",
    "            learning_rate = 0.02, \n",
    "            subsample=params[\"subsample\"],\n",
    "            colsample_bytree=params[\"colsample_bytree\"],\n",
    "            gamma = params[\"gamma\"],\n",
    "            #missing=-1, \n",
    "            eval_metric='auc',\n",
    "            # USE CPU\n",
    "            #nthread=4,\n",
    "            #tree_method='hist' \n",
    "            # USE GPU\n",
    "            tree_method='gpu_hist' \n",
    "        )\n",
    "    print(\"clf:\",clf)        \n",
    "    # Divide in training/validation and test data\n",
    "    #train_df = df[df['TARGET'].notnull()]\n",
    "    #test_df = df[df['TARGET'].isnull()]\n",
    "    print(\"Starting xgboost. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    #del df\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    ## change special character into _ for lightgbm complaining column name error for JSON\n",
    "    train_df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in train_df.columns]\n",
    "    test_df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in test_df.columns]\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']] \n",
    "    x_data = train_df[feats]\n",
    "    y_data = train_df['TARGET']\n",
    "    print(x_data.shape,y_data.shape,\"before k-fold\")\n",
    "    auc_all_folds=[]\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats])):\n",
    "        print(x_data.iloc[train_idx,:].shape)\n",
    "        #print(type(y_data.iloc[train_idx]))        \n",
    "        h = clf.fit(x_data.iloc[train_idx,:], y_data.iloc[train_idx],\n",
    "        \n",
    "            eval_set=[(x_data.iloc[valid_idx,:], y_data.iloc[valid_idx])],\n",
    "            verbose=50, early_stopping_rounds=100)\n",
    "\n",
    "        oof_preds[valid_idx] =clf.predict_proba(x_data.iloc[valid_idx,:])[:,1] ## np array, and predicted prob.\n",
    "        '''\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        '''\n",
    "        auc_from_fold = roc_auc_score(y_data.iloc[valid_idx], oof_preds[valid_idx])\n",
    "        auc_all_folds.append(auc_from_fold)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, auc_from_fold))\n",
    "        #del clf, dtrain, dvalid\n",
    "        gc.collect()        \n",
    "    return oof_preds,auc_from_fold\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters from Bayesian optimization tuning: {'colsample_bytree': 0.8318456460242112, 'gamma': 3.0594505844428506, 'max_depth': 6.163576436526432, 'n_estimators': 1881.2395736545425, 'subsample': 0.8078142393826673}\n",
      "clf: XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8318456460242112,\n",
      "              eval_metric='auc', gamma=3.0594505844428506, gpu_id=None,\n",
      "              importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.02, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=2000, n_jobs=None, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None,\n",
      "              subsample=0.8078142393826673, tree_method='gpu_hist',\n",
      "              validate_parameters=False, verbosity=None)\n",
      "Starting xgboost. Train shape: (20000, 531), test shape: (20000, 531)\n",
      "(20000, 528) (20000,) before k-fold\n",
      "(16000, 528)\n",
      "[0]\tvalidation_0-auc:0.68477\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.72593\n",
      "[100]\tvalidation_0-auc:0.73450\n",
      "[150]\tvalidation_0-auc:0.74165\n",
      "[200]\tvalidation_0-auc:0.74815\n",
      "[250]\tvalidation_0-auc:0.74977\n",
      "[300]\tvalidation_0-auc:0.74955\n",
      "[350]\tvalidation_0-auc:0.75036\n",
      "[400]\tvalidation_0-auc:0.74971\n",
      "[450]\tvalidation_0-auc:0.74999\n",
      "Stopping. Best iteration:\n",
      "[362]\tvalidation_0-auc:0.75071\n",
      "\n",
      "Fold  1 AUC : 0.750712\n",
      "(16000, 528)\n",
      "[0]\tvalidation_0-auc:0.69582\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.72627\n",
      "[100]\tvalidation_0-auc:0.73469\n",
      "[150]\tvalidation_0-auc:0.73963\n",
      "[200]\tvalidation_0-auc:0.74647\n",
      "[250]\tvalidation_0-auc:0.74877\n",
      "[300]\tvalidation_0-auc:0.75171\n",
      "[350]\tvalidation_0-auc:0.75177\n",
      "[400]\tvalidation_0-auc:0.75187\n",
      "[450]\tvalidation_0-auc:0.75148\n",
      "[500]\tvalidation_0-auc:0.75132\n",
      "Stopping. Best iteration:\n",
      "[427]\tvalidation_0-auc:0.75219\n",
      "\n",
      "Fold  2 AUC : 0.752195\n",
      "(16000, 528)\n",
      "[0]\tvalidation_0-auc:0.69719\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.72563\n",
      "[100]\tvalidation_0-auc:0.73634\n",
      "[150]\tvalidation_0-auc:0.74168\n",
      "[200]\tvalidation_0-auc:0.74822\n",
      "[250]\tvalidation_0-auc:0.75104\n",
      "[300]\tvalidation_0-auc:0.75276\n",
      "[350]\tvalidation_0-auc:0.75457\n",
      "[400]\tvalidation_0-auc:0.75519\n",
      "[450]\tvalidation_0-auc:0.75429\n",
      "[500]\tvalidation_0-auc:0.75391\n",
      "Stopping. Best iteration:\n",
      "[413]\tvalidation_0-auc:0.75573\n",
      "\n",
      "Fold  3 AUC : 0.755726\n",
      "(16000, 528)\n",
      "[0]\tvalidation_0-auc:0.70275\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.72865\n",
      "[100]\tvalidation_0-auc:0.73157\n",
      "[150]\tvalidation_0-auc:0.73470\n",
      "[200]\tvalidation_0-auc:0.74122\n",
      "[250]\tvalidation_0-auc:0.74262\n",
      "[300]\tvalidation_0-auc:0.74550\n",
      "[350]\tvalidation_0-auc:0.74785\n",
      "[400]\tvalidation_0-auc:0.74786\n",
      "[450]\tvalidation_0-auc:0.74843\n",
      "[500]\tvalidation_0-auc:0.74836\n",
      "[550]\tvalidation_0-auc:0.74871\n",
      "[600]\tvalidation_0-auc:0.74784\n",
      "Stopping. Best iteration:\n",
      "[540]\tvalidation_0-auc:0.74921\n",
      "\n",
      "Fold  4 AUC : 0.749205\n",
      "(16000, 528)\n",
      "[0]\tvalidation_0-auc:0.69574\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.73579\n",
      "[100]\tvalidation_0-auc:0.73786\n",
      "[150]\tvalidation_0-auc:0.74404\n",
      "[200]\tvalidation_0-auc:0.74772\n",
      "[250]\tvalidation_0-auc:0.75225\n",
      "[300]\tvalidation_0-auc:0.75210\n",
      "Stopping. Best iteration:\n",
      "[246]\tvalidation_0-auc:0.75258\n",
      "\n",
      "Fold  5 AUC : 0.752576\n",
      "Run XGBoost with kfold - done in 19s\n"
     ]
    }
   ],
   "source": [
    "kfold_results=()\n",
    "with timer(\"Run XGBoost with kfold\"):\n",
    "    kfold_results = kfold_xgboost(train_df, test_df, 5, params=xgb_params_best, stratified = False, debug= False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
